{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data19334\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. All changes under this directory will be kept even after reset. Please clean unnecessary files in time to speed up environment loading.\n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "实验要求：\n",
    "1. 观看视频，了解本实验文本分类任务的实现流程；\n",
    "2. 查资料，了解TF/IDF计算方法，并回答在这段文字下面："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **0.导入相关的包**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import os\r\n",
    "import re\r\n",
    "import sklearn\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n",
    "import string\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIDF\r\n",
    "from sklearn.calibration import CalibratedClassifierCV\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "import sklearn.model_selection as sk_model_selection\r\n",
    "#NLTK连不上\r\n",
    "# import nltk\r\n",
    "# from nltk.corpus import stopwords\r\n",
    "# nltk.download('punkt')\r\n",
    "# # nltk.download()#第一次运行可能要nltk.download()\r\n",
    "#PRD：无法使用NLTK的功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **1.切分和清理文本**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Text_process(text,stem=True):\r\n",
    "    #人造特征(网址+电话)\r\n",
    "    man_made_features = [0,0]\r\n",
    "    #是否有网址\r\n",
    "    ###请解释下面的正则表达式\r\n",
    "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')#在注释中解释这一句正则表达式\r\n",
    "    if re.findall(pattern,text):\r\n",
    "        man_made_features[0]=1\r\n",
    "    #是否有电话\r\n",
    "    if re.findall(r'[0-9]{4,}',text):#在注释中解释这一句正则表达式\r\n",
    "        man_made_features[1]=1\r\n",
    "    #去除缩写\"'m\",\"'re\",\"'s\"替换部分缩写\r\n",
    "    short = [\"'m\",\"'re\",\"'s\",\"'ve\",' c u ',\"n't\",' u ']\r\n",
    "    repla = ['','','','have',' see you ',' not',' you ']\r\n",
    "    for i,w in enumerate(short):\r\n",
    "        text=text.replace(w,repla[i])\r\n",
    "    #去除标点\r\n",
    "    remove = str.maketrans('','',string.punctuation) \r\n",
    "    text = text.translate(remove)    \r\n",
    "    #分词\r\n",
    "    # tokens = nltk.word_tokenize(text)\r\n",
    "    tokens = text.split()\r\n",
    "    #去停用词\r\n",
    "    # doc = [w for w in tokens if not w in stopwords.words('english')] \r\n",
    "    doc = tokens\r\n",
    "    #词干提取（默认开启）\r\n",
    "    if stem:\r\n",
    "        s = nltk.stem.SnowballStemmer('english')\r\n",
    "        doc = [s.stem(ws) for ws in doc]\r\n",
    "    result = ' '.join(doc)\r\n",
    "    return man_made_features,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def File_process(filename,stem=True):\r\n",
    "    # 标签\r\n",
    "    labels = []\r\n",
    "    # 特征\r\n",
    "    features=[]\r\n",
    "    # 邮件内容\r\n",
    "    contents = []\r\n",
    "    # 读取文件\r\n",
    "    with open(filename,'r',encoding='utf-8') as f:\r\n",
    "        # 逐行读取\r\n",
    "        lines = f.readlines()\r\n",
    "        i = 0\r\n",
    "        for line in lines:\r\n",
    "            # 得到训练数据和标签\r\n",
    "            target = line.split('\\t')\r\n",
    "            # 读取标签\r\n",
    "            label = 1 if target[0]=='ham' else 0\r\n",
    "            feature,content = Text_process(target[1].lower(),stem)\r\n",
    "            ###\r\n",
    "            labels.append(label)\r\n",
    "            features.append(feature)\r\n",
    "            contents.append(content)\r\n",
    "            #if len(content):\r\n",
    "                # labels.append(label)\r\n",
    "                # features.append(feature)\r\n",
    "                # contents.append(content)\r\n",
    "\r\n",
    "    return contents,features,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **2.利用TFIDF构造训练集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 转换为TFIDF矩阵\r\n",
    "#在注释中解释每一句代码的含义\r\n",
    "def TF_IDF(contents): #  定义TFIDF向量化器，设置参数\r\n",
    "    vec = TFIDF( # 最小文档频率，过滤掉文档频率小于该值的单词\r\n",
    "        min_df=3,  \r\n",
    "        max_features=None, # 最大特征数，保留出现频率最高的前N个特征 \r\n",
    "        strip_accents='unicode', # 去除文本中的重音符号\r\n",
    "        analyzer='word', # 分析单词\r\n",
    "        token_pattern=r'\\w{1,}', # 正则表达式匹配单词\r\n",
    "        ngram_range=(1, 1), # n-gram 特征的范围，这里只考虑单个单词 \r\n",
    "        use_idf=1, # 是否使用idf权重\r\n",
    "        smooth_idf=1, # 平滑idf权重  \r\n",
    "        sublinear_tf=1, # 将tf取对数\r\n",
    "        stop_words='english' # 停用词列表，去除常用词汇\r\n",
    "    )\r\n",
    "    X = vec.fit_transform(contents) # 将输入文本转换为TFIDF矩阵\r\n",
    "    result = pd.DataFrame(X.toarray(),columns=vec.get_feature_names()) # 将矩阵转换为DataFrame\r\n",
    "    print(\"shape of data:\",result.shape) # 打印矩阵的形状 \r\n",
    "    return result,vec # 返回结果和向量化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contents,features,labels=File_process(os.path.join('data','data19334','SMSSpam.txt'),False)#不提取词干，可能要等一会"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat',\n",
       " 'ok lar joking wif you oni',\n",
       " 'free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetc apply 08452810075over18',\n",
       " 'u dun say so early hor you c already then say',\n",
       " 'nah i do not think he goes to usf he lives around here though']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[:5] #前五条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "print(len(contents)) #数据集长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data: (5574, 2581)\n"
     ]
    }
   ],
   "source": [
    "X_1,vec = TF_IDF(contents) # X_1：无人造特征的训练集\r\n",
    "F = pd.DataFrame(features,columns=['网址','电话'])   # F:人造特征\r\n",
    "X_2 = pd.concat([F,X_1],axis=1) # X_2：加入人造特征\r\n",
    "Y = pd.DataFrame(labels,columns=['labels'])#Y:标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   labels\n",
      "0       1\n",
      "1       1\n",
      "2       0\n",
      "3       1\n",
      "4       1\n",
      "无人造特征： (5574, 2581)\n",
      "有人造特征： (5574, 2583)\n"
     ]
    }
   ],
   "source": [
    "print(Y.head(5))\r\n",
    "print(\"无人造特征：\",X_1.shape)\r\n",
    "print(\"有人造特征：\",X_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8659849300322928\n"
     ]
    }
   ],
   "source": [
    "baseline = sum(labels)/len(labels) #0.866 非垃圾邮件占所有邮件的 86.6%，只有结果超过 86.6%，模型才有意义\r\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **3.应用各种模型，对比效果**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#最高分：不提取词干+人造特征(网址+电话)+补集朴素贝叶斯+sigmid校准 准确率： 0.9872643820427529 \r\n",
    "#10折交叉验证\r\n",
    "CV = 10 #交叉验证次数，嫌弃速度慢可以改小\r\n",
    "names = [\"LR\"\r\n",
    "        ,\"LR + Isotonic\"\r\n",
    "        ,\"LR + Sigmoid\"\r\n",
    "        ,\"Naive Bayes\"\r\n",
    "        ]\r\n",
    "\r\n",
    "# 各类模型\r\n",
    "models = [\r\n",
    "        # LR\r\n",
    "        LogisticRegression()\r\n",
    "        # LR + Isotonic\r\n",
    "        ,CalibratedClassifierCV(LogisticRegression(), cv=2, method='isotonic')\r\n",
    "        # LR + Sigmoid\r\n",
    "        ,CalibratedClassifierCV(LogisticRegression(), cv=2, method='sigmoid')\r\n",
    "        # Naive Bayes\r\n",
    "        ,MultinomialNB()\r\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### **对比有无人造特征的效果**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无人造特征\n",
      "LR 交叉验证结果: 0.9660910664530284\n",
      "LR + Isotonic 交叉验证结果: 0.9752398602343583\n",
      "LR + Sigmoid 交叉验证结果: 0.9750603270207139\n",
      "Naive Bayes 交叉验证结果: 0.9786503478053834\n"
     ]
    }
   ],
   "source": [
    "print(\"无人造特征\")\r\n",
    "for name,model in zip(names,models):\r\n",
    "    accs=sum(sk_model_selection.cross_val_score(model, X_1, y=Y,cv=10, n_jobs=-1\r\n",
    "                                                #,scoring='roc_auc'\r\n",
    "                                                ))/10   \r\n",
    "    print(name,'交叉验证结果:',accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有人造特征\n",
      "LR 交叉验证结果: 0.9773932935657612\n",
      "LR + Isotonic 交叉验证结果: 0.9838529500717488\n",
      "LR + Sigmoid 交叉验证结果: 0.9779315714625844\n",
      "Naive Bayes 交叉验证结果: 0.9874413621358661\n"
     ]
    }
   ],
   "source": [
    "print(\"有人造特征\")\r\n",
    "for name,model in zip(names,models):\r\n",
    "    accs=sum(sk_model_selection.cross_val_score(model, X_2, y=Y,cv=CV, n_jobs=-1\r\n",
    "                                                #,scoring='roc_auc'\r\n",
    "                                                ))/10    \r\n",
    "    print(name,'交叉验证结果:',accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
