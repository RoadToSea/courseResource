{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "# 定义一个数据加载的函数load_data，用于数据生成和格式转换。\r\n",
    "import paddle\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "def load_data(num_steps=10):\r\n",
    "    heads=[[1,2],[2,4],[3,2],[3,1]]\r\n",
    "    labels = [[3], [6], [5], [4]]\r\n",
    "    # 装配数据\r\n",
    "    samples = []\r\n",
    "    for idx, head in enumerate(heads):\r\n",
    "        seq = [head+[0]*(num_steps-len(heads[0]))]\r\n",
    "        seq = paddle.to_tensor(seq, dtype=\"int64\")\r\n",
    "        label = paddle.to_tensor(labels[idx], dtype=\"float32\")\r\n",
    "        yield seq, label\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 一、简单RNN网络搭建\r\n",
    "# 简单RNN网络的代码实现如下，这里只保留了最后一个时刻的RNN输出向量，用于完成接下来的数字预测实验。\r\n",
    "\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "\r\n",
    "# 声明RNN网络和相关参数\r\n",
    "class SelfRNN(paddle.nn.Layer):\r\n",
    "    def __init__(self, emb_size, hidden_size):\r\n",
    "        super(SelfRNN, self).__init__()\r\n",
    "        self.emb_size = emb_size\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.W = paddle.create_parameter(shape=[emb_size, hidden_size], dtype=\"float32\")\r\n",
    "        self.U = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\")\r\n",
    "        self.b = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\")\r\n",
    "\r\n",
    "    # 定义前向计算\r\n",
    "    def forward(self, inputs):\r\n",
    "        # inputs batch_size x seq_len x emb_dim\r\n",
    "        batch_size, seq_len, emb_dim = inputs.shape\r\n",
    "\r\n",
    "        # 初始化向量\r\n",
    "        hidden_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=\"float32\")\r\n",
    "        # 执行RNN计算\r\n",
    "        for step in range(seq_len):\r\n",
    "            step_input = inputs[:, step, :]\r\n",
    "            hidden_state = F.tanh(paddle.matmul(step_input, self.W) + paddle.matmul(hidden_state, self.U) + self.b)\r\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义数据预测模型\r\n",
    "# 定义一个数字预测模型NumericPrediction，基于RNN网络处理数字序列，并使用最后时刻的状态向量进行数字标签预测。\r\n",
    "\r\n",
    "# 模型定义\r\n",
    "class NumericPrediction(paddle.nn.Layer):\r\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, model_type=\"RNN\"):\r\n",
    "        super(NumericPrediction, self).__init__()\r\n",
    "        self.model_type = model_type\r\n",
    "        self.model = SelfRNN(emb_size, hidden_size) if model_type == \"RNN\" else SelfLSTM(emb_size, hidden_size)\r\n",
    "        self.embedding = paddle.nn.Embedding(vocab_size, emb_size)\r\n",
    "        self.cls_fc = paddle.nn.Linear(hidden_size, 1)\r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        inputs_emb = self.embedding(inputs)\r\n",
    "\r\n",
    "        state = self.model(inputs_emb)\r\n",
    "        hidden_state = state if self.model_type == \"RNN\" else state[1]\r\n",
    "\r\n",
    "        logits = self.cls_fc(hidden_state)\r\n",
    "\r\n",
    "        return logits, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1129 22:11:58.893406    98 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 10.1\n",
      "W1129 22:11:58.899405    98 device_context.cc:372] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "# 训练配置\r\n",
    "# 配置模型参数（如：训练轮次、学习率等）、训练资源、实例化模型并指定优化器。\r\n",
    "# 学习率是优化器的一个参数，代表参数更新幅度的大小，即步长。当学习率最优时，模型的有效容量最大，最终能达到的效果最好。\r\n",
    "# SGD是比较成熟的优化算法之一，每次训练少量数据，基于这部分数据计算梯度和损失来更新参数。\r\n",
    "\r\n",
    "# 训练配置\r\n",
    "paddle.seed(0)\r\n",
    "np.random.seed(0)\r\n",
    "random.seed(0)\r\n",
    "\r\n",
    "# 设置模型参数\r\n",
    "epochs = 5\r\n",
    "learning_rate = 0.05\r\n",
    "batch_size = 1\r\n",
    "num_steps = 10\r\n",
    "\r\n",
    "vocab_size = 10\r\n",
    "emb_size = 128\r\n",
    "hidden_size = 128\r\n",
    "\r\n",
    "# 指定训练资源\r\n",
    "use_gpu = True if paddle.get_device().startswith(\"gpu\") else False\r\n",
    "if use_gpu:\r\n",
    "    paddle.set_device('gpu:0')\r\n",
    "\r\n",
    "# 实例化模型\r\n",
    "model = NumericPrediction(vocab_size, emb_size, hidden_size, model_type=\"RNN\")\r\n",
    "\r\n",
    "# 指定优化器\r\n",
    "optimizer = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.SelfRNN'>\n",
      "=========epoch: 1, step: 0, loss: 6.86142========\n",
      "W_grad_l2: 14.512397, U_grad_l2: 38.835285, b_grad_l2: 11.312019 \n",
      "=========epoch: 1, step: 1, loss: 1.07215========\n",
      "W_grad_l2: 10.520271, U_grad_l2: 50.651566, b_grad_l2: 7.457427 \n",
      "=========epoch: 1, step: 2, loss: 113.24744========\n",
      "W_grad_l2: 20.071827, U_grad_l2: 125.992210, b_grad_l2: 12.833219 \n",
      "=========epoch: 1, step: 3, loss: 3844.58228========\n",
      "W_grad_l2: 931.636597, U_grad_l2: 5998.811035, b_grad_l2: 601.230713 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 2, step: 0, loss: 1827.26697========\n",
      "W_grad_l2: 24272.710938, U_grad_l2: 7647.982422, b_grad_l2: 701.979187 \n",
      "=========epoch: 2, step: 1, loss: 94880.67188========\n",
      "W_grad_l2: 230114.937500, U_grad_l2: 57992.808594, b_grad_l2: 5152.492188 \n",
      "=========epoch: 2, step: 2, loss: 9347954.00000========\n",
      "W_grad_l2: 26287252.000000, U_grad_l2: 1054054.125000, b_grad_l2: 93402.585938 \n",
      "=========epoch: 2, step: 3, loss: 112143856.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 3, step: 0, loss: 15880941568.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 3, step: 1, loss: 2248909520896.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 3, step: 2, loss: 318468167565312.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 3, step: 3, loss: 45098277594464256.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 4, step: 0, loss: 6386365955378774016.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 4, step: 1, loss: 904373118828082626560.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 4, step: 2, loss: 128068295149981746069504.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 4, step: 3, loss: 18135759638742919140081664.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 5, step: 0, loss: 2568204409346198369195786240.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 5, step: 1, loss: 363683269934853220227914661888.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 5, step: 2, loss: 51501182877722502253239368417280.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 5, step: 3, loss: 7293081724172137597724447586910208.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:143: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if data.dtype == np.object:\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\r\n",
    "# 在训练过程中，每轮迭代打印一次训练结果，观察Loss和RNN中参数W、U和b的梯度信息。\r\n",
    "# 为了方便观察模型训练效果，计算梯度矩阵的L2范数进行展示，L2范数越大，代表梯度矩阵中的值也越大，越倾向产生梯度爆炸。\r\n",
    "\r\n",
    "from numpy.linalg import norm\r\n",
    "\r\n",
    "def output_grads_l2(model, hidden_state):\r\n",
    "\r\n",
    "    W_grad_l2, U_grad_l2, b_grad_l2 = 0, 0, 0\r\n",
    "    for name, param in model.named_parameters(): \r\n",
    "        if name == \"model.W\":  \r\n",
    "            W_grad_l2 = norm(param.grad)\r\n",
    "        if name == \"model.U\": \r\n",
    "            U_grad_l2 = norm(param.grad)\r\n",
    "        if name == \"model.b\": \r\n",
    "            b_grad_l2 = norm(param.grad)\r\n",
    "\r\n",
    "    return W_grad_l2, U_grad_l2, b_grad_l2\r\n",
    "\r\n",
    "\r\n",
    "# 开始训练\r\n",
    "def train(model, logging_steps=1):\r\n",
    "    model.train()\r\n",
    "    print(type(model.model))\r\n",
    "    global_step = 0\r\n",
    "\r\n",
    "    for epoch in range(1, epochs+1):\r\n",
    "        total_count = 0\r\n",
    "        correct_count = 0\r\n",
    "        for step, batch in enumerate(load_data(num_steps=num_steps)):\r\n",
    "            global_step += 1\r\n",
    "            batch_seq, batch_label = batch\r\n",
    "            predicts, hidden_state = model(batch_seq)\r\n",
    "            loss = F.mse_loss(predicts, batch_label)\r\n",
    "\r\n",
    "            loss.backward()\r\n",
    "\r\n",
    "            if global_step % logging_steps ==0:\r\n",
    "                print(\"=========epoch: %d, step: %d, loss: %.5f========\" % (epoch, step, loss))\r\n",
    "                W_grad_l2, U_grad_l2, b_grad_l2 = output_grads_l2(model, hidden_state)\r\n",
    "                print(\"W_grad_l2: %f, U_grad_l2: %f, b_grad_l2: %f \" % (W_grad_l2, U_grad_l2, b_grad_l2))\r\n",
    "\r\n",
    "            optimizer.step()\r\n",
    "            optimizer.clear_grad()\r\n",
    "\r\n",
    "            with paddle.no_grad():\r\n",
    "                # 检验是否预测正确\r\n",
    "                predicts = predicts.squeeze(1)\r\n",
    "                diff = (predicts - batch_label).abs()\r\n",
    "                correct_count += paddle.cast(diff<0.1, \"int64\").sum().numpy()[0]\r\n",
    "                total_count += len(batch_seq)\r\n",
    "\r\n",
    "        if global_step % logging_steps ==0:\r\n",
    "            acc = correct_count/total_count\r\n",
    "            print(\"\\ncorrect/total:%d/%d, Accuracy: %.5f \\n\" % (correct_count, total_count, acc))\r\n",
    "\r\n",
    "\r\n",
    "train(model, logging_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.SelfRNN'>\n",
      "=========epoch: 5, step: 3, loss: 0.08996========\n",
      "W_grad_l2: 0.937741, U_grad_l2: 3.769030, b_grad_l2: 0.659802 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 10, step: 3, loss: 0.03137========\n",
      "W_grad_l2: 0.755688, U_grad_l2: 2.298628, b_grad_l2: 0.449241 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 15, step: 3, loss: 0.03329========\n",
      "W_grad_l2: 1.904926, U_grad_l2: 4.428648, b_grad_l2: 0.870908 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 20, step: 3, loss: 4.07880========\n",
      "W_grad_l2: 15.657574, U_grad_l2: 43.209564, b_grad_l2: 9.585383 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 25, step: 3, loss: 0.11522========\n",
      "W_grad_l2: 2.398134, U_grad_l2: 6.604366, b_grad_l2: 1.389267 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 30, step: 3, loss: 0.08942========\n",
      "W_grad_l2: 1.720007, U_grad_l2: 5.059711, b_grad_l2: 0.988197 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 35, step: 3, loss: 0.29813========\n",
      "W_grad_l2: 2.682343, U_grad_l2: 8.338731, b_grad_l2: 1.448459 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 40, step: 3, loss: 0.73660========\n",
      "W_grad_l2: 7.525804, U_grad_l2: 20.784595, b_grad_l2: 4.362229 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 45, step: 3, loss: 2.57438========\n",
      "W_grad_l2: 10.360266, U_grad_l2: 32.414787, b_grad_l2: 6.836927 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 50, step: 3, loss: 0.01281========\n",
      "W_grad_l2: 0.513162, U_grad_l2: 1.657599, b_grad_l2: 0.288512 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 55, step: 3, loss: 0.04818========\n",
      "W_grad_l2: 1.276919, U_grad_l2: 4.007478, b_grad_l2: 0.733364 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 60, step: 3, loss: 0.19252========\n",
      "W_grad_l2: 1.934360, U_grad_l2: 6.775870, b_grad_l2: 1.104031 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 65, step: 3, loss: 0.32606========\n",
      "W_grad_l2: 3.036144, U_grad_l2: 10.036167, b_grad_l2: 1.700709 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 70, step: 3, loss: 1.25917========\n",
      "W_grad_l2: 3.531654, U_grad_l2: 13.911977, b_grad_l2: 2.185620 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 75, step: 3, loss: 0.52587========\n",
      "W_grad_l2: 3.632885, U_grad_l2: 12.945422, b_grad_l2: 2.365465 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 80, step: 3, loss: 0.00064========\n",
      "W_grad_l2: 0.090386, U_grad_l2: 0.364184, b_grad_l2: 0.061383 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 85, step: 3, loss: 0.08856========\n",
      "W_grad_l2: 1.202377, U_grad_l2: 4.660611, b_grad_l2: 0.782934 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 90, step: 3, loss: 0.31382========\n",
      "W_grad_l2: 1.971171, U_grad_l2: 7.683953, b_grad_l2: 1.323610 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 95, step: 3, loss: 1.15394========\n",
      "W_grad_l2: 2.944917, U_grad_l2: 12.643519, b_grad_l2: 1.965563 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 100, step: 3, loss: 0.26171========\n",
      "W_grad_l2: 2.092007, U_grad_l2: 8.183855, b_grad_l2: 1.413778 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 105, step: 3, loss: 0.08576========\n",
      "W_grad_l2: 1.062744, U_grad_l2: 4.363291, b_grad_l2: 0.707196 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 110, step: 3, loss: 0.39172========\n",
      "W_grad_l2: 1.930986, U_grad_l2: 8.144922, b_grad_l2: 1.315343 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 115, step: 3, loss: 0.03309========\n",
      "W_grad_l2: 0.682325, U_grad_l2: 2.780651, b_grad_l2: 0.499025 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 120, step: 3, loss: 0.20289========\n",
      "W_grad_l2: 1.514188, U_grad_l2: 6.426356, b_grad_l2: 1.115668 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 125, step: 3, loss: 0.00177========\n",
      "W_grad_l2: 0.135403, U_grad_l2: 0.597992, b_grad_l2: 0.093225 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 130, step: 3, loss: 0.23189========\n",
      "W_grad_l2: 1.620687, U_grad_l2: 6.890719, b_grad_l2: 1.200420 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 135, step: 3, loss: 0.24358========\n",
      "W_grad_l2: 1.777457, U_grad_l2: 7.668630, b_grad_l2: 1.334610 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 140, step: 3, loss: 0.48665========\n",
      "W_grad_l2: 2.270480, U_grad_l2: 10.344738, b_grad_l2: 1.662366 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 145, step: 3, loss: 0.24929========\n",
      "W_grad_l2: 1.669027, U_grad_l2: 7.586175, b_grad_l2: 1.231455 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 150, step: 3, loss: 0.02307========\n",
      "W_grad_l2: 0.438883, U_grad_l2: 2.078967, b_grad_l2: 0.336443 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 155, step: 3, loss: 0.17424========\n",
      "W_grad_l2: 1.201487, U_grad_l2: 5.751534, b_grad_l2: 0.882514 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 160, step: 3, loss: 0.26444========\n",
      "W_grad_l2: 1.337326, U_grad_l2: 6.398064, b_grad_l2: 0.989050 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 165, step: 3, loss: 0.95313========\n",
      "W_grad_l2: 2.096759, U_grad_l2: 10.919316, b_grad_l2: 1.607420 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 170, step: 3, loss: 0.08107========\n",
      "W_grad_l2: 0.978191, U_grad_l2: 4.602089, b_grad_l2: 0.776379 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 175, step: 3, loss: 0.85845========\n",
      "W_grad_l2: 2.860303, U_grad_l2: 14.216789, b_grad_l2: 2.301762 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 180, step: 3, loss: 0.37206========\n",
      "W_grad_l2: 1.682390, U_grad_l2: 8.200567, b_grad_l2: 1.319599 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 185, step: 3, loss: 0.08009========\n",
      "W_grad_l2: 0.692717, U_grad_l2: 3.463679, b_grad_l2: 0.514662 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 190, step: 3, loss: 0.00192========\n",
      "W_grad_l2: 0.118320, U_grad_l2: 0.591420, b_grad_l2: 0.087195 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 195, step: 3, loss: 0.01343========\n",
      "W_grad_l2: 0.277881, U_grad_l2: 1.402805, b_grad_l2: 0.211539 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 200, step: 3, loss: 0.19505========\n",
      "W_grad_l2: 1.101581, U_grad_l2: 5.726313, b_grad_l2: 0.848269 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 205, step: 3, loss: 0.43471========\n",
      "W_grad_l2: 1.327780, U_grad_l2: 7.081885, b_grad_l2: 1.012012 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 210, step: 3, loss: 0.04626========\n",
      "W_grad_l2: 0.506045, U_grad_l2: 2.709474, b_grad_l2: 0.402394 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 215, step: 3, loss: 0.12257========\n",
      "W_grad_l2: 0.756005, U_grad_l2: 3.992998, b_grad_l2: 0.606918 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 220, step: 3, loss: 0.01623========\n",
      "W_grad_l2: 0.289250, U_grad_l2: 1.520755, b_grad_l2: 0.231214 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 225, step: 3, loss: 0.14530========\n",
      "W_grad_l2: 0.904590, U_grad_l2: 4.850666, b_grad_l2: 0.721815 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 230, step: 3, loss: 0.72237========\n",
      "W_grad_l2: 1.489455, U_grad_l2: 8.321208, b_grad_l2: 1.167468 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 235, step: 3, loss: 0.68798========\n",
      "W_grad_l2: 2.018872, U_grad_l2: 10.777289, b_grad_l2: 1.678269 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 240, step: 3, loss: 0.32061========\n",
      "W_grad_l2: 1.024993, U_grad_l2: 5.649352, b_grad_l2: 0.805657 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 245, step: 3, loss: 0.11643========\n",
      "W_grad_l2: 0.793299, U_grad_l2: 4.223741, b_grad_l2: 0.626147 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 250, step: 3, loss: 0.07479========\n",
      "W_grad_l2: 0.651522, U_grad_l2: 3.443592, b_grad_l2: 0.512163 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 255, step: 3, loss: 0.09455========\n",
      "W_grad_l2: 0.620565, U_grad_l2: 3.435652, b_grad_l2: 0.486479 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 260, step: 3, loss: 0.05169========\n",
      "W_grad_l2: 0.498871, U_grad_l2: 2.709066, b_grad_l2: 0.384551 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 265, step: 3, loss: 0.01378========\n",
      "W_grad_l2: 0.257599, U_grad_l2: 1.363538, b_grad_l2: 0.206163 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 270, step: 3, loss: 0.03037========\n",
      "W_grad_l2: 0.393258, U_grad_l2: 2.118340, b_grad_l2: 0.314125 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 275, step: 3, loss: 0.11899========\n",
      "W_grad_l2: 0.778949, U_grad_l2: 4.117970, b_grad_l2: 0.632712 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 280, step: 3, loss: 0.00394========\n",
      "W_grad_l2: 0.111352, U_grad_l2: 0.634379, b_grad_l2: 0.086930 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 285, step: 3, loss: 0.00036========\n",
      "W_grad_l2: 0.035823, U_grad_l2: 0.195546, b_grad_l2: 0.028408 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 290, step: 3, loss: 0.03376========\n",
      "W_grad_l2: 0.374248, U_grad_l2: 2.049002, b_grad_l2: 0.295449 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 295, step: 3, loss: 0.46993========\n",
      "W_grad_l2: 1.081501, U_grad_l2: 6.203219, b_grad_l2: 0.872866 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 300, step: 3, loss: 0.00068========\n",
      "W_grad_l2: 0.054460, U_grad_l2: 0.296330, b_grad_l2: 0.044264 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 305, step: 3, loss: 0.28100========\n",
      "W_grad_l2: 0.921435, U_grad_l2: 5.114975, b_grad_l2: 0.759346 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 310, step: 3, loss: 0.02668========\n",
      "W_grad_l2: 0.276211, U_grad_l2: 1.583142, b_grad_l2: 0.231156 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 315, step: 3, loss: 0.15016========\n",
      "W_grad_l2: 0.594877, U_grad_l2: 3.514594, b_grad_l2: 0.478773 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 320, step: 3, loss: 0.05971========\n",
      "W_grad_l2: 0.461181, U_grad_l2: 2.599986, b_grad_l2: 0.365814 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 325, step: 3, loss: 0.03435========\n",
      "W_grad_l2: 0.337746, U_grad_l2: 1.847187, b_grad_l2: 0.274324 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 330, step: 3, loss: 0.23924========\n",
      "W_grad_l2: 0.924656, U_grad_l2: 5.174083, b_grad_l2: 0.766856 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 335, step: 3, loss: 0.03719========\n",
      "W_grad_l2: 0.277888, U_grad_l2: 1.636867, b_grad_l2: 0.228573 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 340, step: 3, loss: 0.07530========\n",
      "W_grad_l2: 0.501795, U_grad_l2: 2.778950, b_grad_l2: 0.397845 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 345, step: 3, loss: 0.30984========\n",
      "W_grad_l2: 1.201689, U_grad_l2: 6.127521, b_grad_l2: 0.991249 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 350, step: 3, loss: 0.40259========\n",
      "W_grad_l2: 1.123299, U_grad_l2: 6.381588, b_grad_l2: 0.913799 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 355, step: 3, loss: 0.22149========\n",
      "W_grad_l2: 0.629830, U_grad_l2: 3.739282, b_grad_l2: 0.499540 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 360, step: 3, loss: 0.03788========\n",
      "W_grad_l2: 0.304875, U_grad_l2: 1.787912, b_grad_l2: 0.241714 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 365, step: 3, loss: 0.07388========\n",
      "W_grad_l2: 0.368321, U_grad_l2: 2.171045, b_grad_l2: 0.294302 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 370, step: 3, loss: 0.05316========\n",
      "W_grad_l2: 0.338285, U_grad_l2: 1.927482, b_grad_l2: 0.261527 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 375, step: 3, loss: 0.16662========\n",
      "W_grad_l2: 0.544310, U_grad_l2: 3.227052, b_grad_l2: 0.431725 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 380, step: 3, loss: 0.00394========\n",
      "W_grad_l2: 0.097328, U_grad_l2: 0.556701, b_grad_l2: 0.074718 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 385, step: 3, loss: 0.32371========\n",
      "W_grad_l2: 0.700686, U_grad_l2: 4.199043, b_grad_l2: 0.541373 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 390, step: 3, loss: 0.01552========\n",
      "W_grad_l2: 0.187028, U_grad_l2: 1.078578, b_grad_l2: 0.144897 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 395, step: 3, loss: 0.23181========\n",
      "W_grad_l2: 0.604394, U_grad_l2: 3.580111, b_grad_l2: 0.481006 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 400, step: 3, loss: 0.00864========\n",
      "W_grad_l2: 0.146711, U_grad_l2: 0.820990, b_grad_l2: 0.114992 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 405, step: 3, loss: 0.16089========\n",
      "W_grad_l2: 0.530893, U_grad_l2: 3.004562, b_grad_l2: 0.424202 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 410, step: 3, loss: 0.02471========\n",
      "W_grad_l2: 0.272244, U_grad_l2: 1.444738, b_grad_l2: 0.214412 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 415, step: 3, loss: 0.37722========\n",
      "W_grad_l2: 0.772773, U_grad_l2: 4.350424, b_grad_l2: 0.612301 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 420, step: 3, loss: 0.14282========\n",
      "W_grad_l2: 0.623690, U_grad_l2: 3.346331, b_grad_l2: 0.493924 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 425, step: 3, loss: 0.25321========\n",
      "W_grad_l2: 0.587008, U_grad_l2: 3.446850, b_grad_l2: 0.473935 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 430, step: 3, loss: 0.03621========\n",
      "W_grad_l2: 0.267754, U_grad_l2: 1.493575, b_grad_l2: 0.209620 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 435, step: 3, loss: 0.37897========\n",
      "W_grad_l2: 0.749600, U_grad_l2: 4.148187, b_grad_l2: 0.591963 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 440, step: 3, loss: 0.16470========\n",
      "W_grad_l2: 0.653913, U_grad_l2: 3.480838, b_grad_l2: 0.514274 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 445, step: 3, loss: 0.04856========\n",
      "W_grad_l2: 0.253448, U_grad_l2: 1.452862, b_grad_l2: 0.203155 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 450, step: 3, loss: 0.04169========\n",
      "W_grad_l2: 0.299248, U_grad_l2: 1.641793, b_grad_l2: 0.222303 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 455, step: 3, loss: 0.02261========\n",
      "W_grad_l2: 0.199171, U_grad_l2: 1.069202, b_grad_l2: 0.150605 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 460, step: 3, loss: 0.09863========\n",
      "W_grad_l2: 0.446039, U_grad_l2: 2.445713, b_grad_l2: 0.341724 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 465, step: 3, loss: 0.05528========\n",
      "W_grad_l2: 0.358752, U_grad_l2: 1.877517, b_grad_l2: 0.263023 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 470, step: 3, loss: 0.00688========\n",
      "W_grad_l2: 0.095729, U_grad_l2: 0.546348, b_grad_l2: 0.073197 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 475, step: 3, loss: 0.22227========\n",
      "W_grad_l2: 0.632048, U_grad_l2: 3.550288, b_grad_l2: 0.485691 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 480, step: 3, loss: 0.18565========\n",
      "W_grad_l2: 0.447283, U_grad_l2: 2.586938, b_grad_l2: 0.348072 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 485, step: 3, loss: 0.02294========\n",
      "W_grad_l2: 0.198650, U_grad_l2: 1.088267, b_grad_l2: 0.148158 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 490, step: 3, loss: 0.09343========\n",
      "W_grad_l2: 0.474024, U_grad_l2: 2.404890, b_grad_l2: 0.359847 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 495, step: 3, loss: 0.04526========\n",
      "W_grad_l2: 0.266613, U_grad_l2: 1.433389, b_grad_l2: 0.213252 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 500, step: 3, loss: 0.06260========\n",
      "W_grad_l2: 0.381162, U_grad_l2: 1.907166, b_grad_l2: 0.290186 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RNN网络优化：梯度截断\r\n",
    "# 针对RNN梯度爆炸的情况，可以采用梯度截断的方式缓解，当梯度达到一定阈值时，对其进行截断。\r\n",
    "# 一般截断有两种方式：按值截断和按模截断。\r\n",
    "# 本实验采用按模截断的方式，使用 ClipGradByGlobalNorm API。在代码实现时，将ClipGradByNorm传入优化器，优化器在反向迭代过程中，每次梯度更新时便可以梯度裁剪。\r\n",
    "\r\n",
    "# 训练配置\r\n",
    "paddle.seed(0)\r\n",
    "np.random.seed(0)\r\n",
    "random.seed(0)\r\n",
    "\r\n",
    "# 设置模型参数\r\n",
    "epochs = 500\r\n",
    "learning_rate = 0.05\r\n",
    "batch_size = 1\r\n",
    "num_steps = 10\r\n",
    "\r\n",
    "vocab_size = 10\r\n",
    "emb_size = 128\r\n",
    "hidden_size = 128\r\n",
    "\r\n",
    "# 指定训练资源\r\n",
    "use_gpu = True if paddle.get_device().startswith(\"gpu\") else False\r\n",
    "if use_gpu:\r\n",
    "    paddle.set_device('gpu:0')\r\n",
    "\r\n",
    "# 实例化模型\r\n",
    "model = NumericPrediction(vocab_size, emb_size, hidden_size, model_type=\"RNN\")\r\n",
    "\r\n",
    "# 指定优化器\r\n",
    "clip = paddle.nn.ClipGradByNorm(clip_norm=1.0)\r\n",
    "optimizer = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=model.parameters(), weight_decay=0.,grad_clip=clip)\r\n",
    "\r\n",
    "#训练模型\r\n",
    "train(model, logging_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LSTM网络搭建\r\n",
    "# LSTM网络的代码实现与RNN结构相似，只是在RNN的基础上增加了隐藏门、输入门、遗忘门的定义和计算。这里依然选择保留序列的最后一个单词位置的输出向量。\r\n",
    "\r\n",
    "# 导入paddle\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "\r\n",
    "# 声明LSTM网络和相关参数\r\n",
    "class SelfLSTM(paddle.nn.Layer):\r\n",
    "    def __init__(self, emb_dim, hidden_size):\r\n",
    "        super(SelfLSTM, self).__init__()\r\n",
    "        self.emb_dim = emb_dim\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.w_i = paddle.create_parameter(shape=[emb_dim, hidden_size], dtype=\"float32\")\r\n",
    "        self.w_f = paddle.create_parameter(shape=[emb_dim, hidden_size], dtype=\"float32\")\r\n",
    "        self.w_o = paddle.create_parameter(shape=[emb_dim, hidden_size], dtype=\"float32\")\r\n",
    "        self.w_a = paddle.create_parameter(shape=[emb_dim, hidden_size], dtype=\"float32\")\r\n",
    "        self.u_i = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\")\r\n",
    "        self.u_f = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\")\r\n",
    "        self.u_o = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\")\r\n",
    "        self.u_a = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\")\r\n",
    "        self.b_i = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\")\r\n",
    "        self.b_f = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\")\r\n",
    "        self.b_o = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\")\r\n",
    "        self.b_a = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\")\r\n",
    "\r\n",
    "    # 定义前向计算\r\n",
    "    def forward(self, inputs):\r\n",
    "        # inputs batch_size x seq_len x emb_dim\r\n",
    "        batch_size, seq_len, emb_dim = inputs.shape\r\n",
    "\r\n",
    "        # 初始化状态向量和隐状态向量\r\n",
    "        cell_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=\"float32\")\r\n",
    "        hidden_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=\"float32\")\r\n",
    "\r\n",
    "        # 执行LSTM计算，包括：隐藏门、输入门、遗忘门、候选状态向量、状态向量和隐状态向量\r\n",
    "        for step in range(seq_len):\r\n",
    "            input_step = inputs[:, step, :]\r\n",
    "            i = F.sigmoid(paddle.matmul(input_step, self.w_i) + paddle.matmul(hidden_state, self.u_i) + self.b_i)\r\n",
    "            f = F.sigmoid(paddle.matmul(input_step, self.w_f) + paddle.matmul(hidden_state, self.u_f) + self.b_f)\r\n",
    "            o = F.sigmoid(paddle.matmul(input_step, self.w_o) + paddle.matmul(hidden_state, self.u_o) + self.b_o)\r\n",
    "            c_tilde = F.tanh(paddle.matmul(input_step, self.w_a) + paddle.matmul(hidden_state, self.u_a) + self.b_a)\r\n",
    "            cell_state = f * cell_state + i * c_tilde\r\n",
    "            hidden_state = o * F.tanh(cell_state)\r\n",
    "\r\n",
    "        return cell_state, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.SelfLSTM'>\n",
      "=========epoch: 20, step: 3, loss: 0.99843========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 40, step: 3, loss: 0.87798========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 60, step: 3, loss: 0.76418========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 80, step: 3, loss: 0.70357========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 100, step: 3, loss: 0.01129========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 120, step: 3, loss: 1.79431========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 140, step: 3, loss: 0.00962========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 160, step: 3, loss: 0.06936========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 180, step: 3, loss: 0.00924========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 200, step: 3, loss: 0.00670========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 220, step: 3, loss: 0.05455========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 240, step: 3, loss: 0.00227========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 260, step: 3, loss: 0.00026========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 280, step: 3, loss: 0.00003========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 300, step: 3, loss: 0.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 320, step: 3, loss: 0.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 340, step: 3, loss: 0.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 360, step: 3, loss: 0.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 380, step: 3, loss: 0.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 400, step: 3, loss: 0.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 420, step: 3, loss: 0.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 440, step: 3, loss: 0.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 460, step: 3, loss: 0.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 480, step: 3, loss: 0.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 500, step: 3, loss: 0.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 基于LSTM网络进行数字预测\r\n",
    "# 实验过程中尽量复用RNN的参数配置，模型实例化方面需要将数字预测模型NumericPrediction中的model_type设置为LSTM，进行模型训练和评估。\r\n",
    "# 在训练过程中统计模型预测的准确率，假如模型预测的数值和原本标签数据的差值在[-0.1,0.1]之内，则认为预测正确，否则认为预测错误。\r\n",
    "\r\n",
    "# 训练配置\r\n",
    "paddle.seed(0)\r\n",
    "np.random.seed(0)\r\n",
    "random.seed(0)\r\n",
    "\r\n",
    "# 设置模型参数\r\n",
    "epochs = 500\r\n",
    "learning_rate = 0.1\r\n",
    "batch_size = 1\r\n",
    "num_steps = 10\r\n",
    "\r\n",
    "vocab_size = 10\r\n",
    "emb_size = 128\r\n",
    "hidden_size = 128\r\n",
    "\r\n",
    "# 指定训练资源\r\n",
    "use_gpu = True if paddle.get_device().startswith(\"gpu\") else False\r\n",
    "if use_gpu:\r\n",
    "    paddle.set_device('gpu:0')\r\n",
    "\r\n",
    "# 实例化模型\r\n",
    "model = NumericPrediction(vocab_size, emb_size, hidden_size, model_type=\"LSTM\")\r\n",
    "\r\n",
    "# 指定优化器\r\n",
    "clip = paddle.nn.ClipGradByNorm(clip_norm=1.0)\r\n",
    "optimizer = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=model.parameters(), grad_clip=clip)\r\n",
    "\r\n",
    "train(model, logging_steps=80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
