{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "# 定义一个数据加载的函数load_data，用于数据生成和格式转换。\r\n",
    "import paddle\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "def load_data(num_steps=10):\r\n",
    "    heads=[[1,2],[2,4],[3,2],[3,1]]\r\n",
    "    labels = [[3], [6], [5], [4]]\r\n",
    "    # 装配数据\r\n",
    "    samples = []\r\n",
    "    for idx, head in enumerate(heads):\r\n",
    "        seq = [head+[0]*(num_steps-len(heads[0]))]\r\n",
    "        seq = paddle.to_tensor(seq, dtype=\"int64\")\r\n",
    "        label = paddle.to_tensor(labels[idx], dtype=\"float32\")\r\n",
    "        yield seq, label\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 一、简单RNN网络搭建\r\n",
    "# 简单RNN网络的代码实现如下，这里只保留了最后一个时刻的RNN输出向量，用于完成接下来的数字预测实验。\r\n",
    "\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "\r\n",
    "# 声明RNN网络和相关参数\r\n",
    "class SelfRNN(paddle.nn.Layer):\r\n",
    "    def __init__(self, emb_size, hidden_size):\r\n",
    "        super(SelfRNN, self).__init__()\r\n",
    "        self.emb_size = emb_size\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.W = paddle.create_parameter(shape=[emb_size, hidden_size], dtype=\"float32\")\r\n",
    "        self.U = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\")\r\n",
    "        self.b = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\")\r\n",
    "\r\n",
    "    # 定义前向计算\r\n",
    "    def forward(self, inputs):\r\n",
    "        # inputs batch_size x seq_len x emb_dim\r\n",
    "        batch_size, seq_len, emb_dim = inputs.shape\r\n",
    "\r\n",
    "        # 初始化向量\r\n",
    "        hidden_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=\"float32\")\r\n",
    "        # 执行RNN计算\r\n",
    "        for step in range(seq_len):\r\n",
    "            step_input = inputs[:, step, :]\r\n",
    "            hidden_state = F.tanh(paddle.matmul(step_input, self.W) + paddle.matmul(hidden_state, self.U) + self.b)\r\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义数据预测模型\r\n",
    "# 定义一个数字预测模型NumericPrediction，基于RNN网络处理数字序列，并使用最后时刻的状态向量进行数字标签预测。\r\n",
    "\r\n",
    "# 模型定义\r\n",
    "class NumericPrediction(paddle.nn.Layer):\r\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, model_type=\"RNN\"):\r\n",
    "        super(NumericPrediction, self).__init__()\r\n",
    "        self.model_type = model_type\r\n",
    "        self.model = SelfRNN(emb_size, hidden_size) if model_type == \"RNN\" else SelfLSTM(emb_size, hidden_size)\r\n",
    "        self.embedding = paddle.nn.Embedding(vocab_size, emb_size)\r\n",
    "        self.cls_fc = paddle.nn.Linear(hidden_size, 1)\r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        inputs_emb = self.embedding(inputs)\r\n",
    "\r\n",
    "        state = self.model(inputs_emb)\r\n",
    "        hidden_state = state if self.model_type == \"RNN\" else state[1]\r\n",
    "\r\n",
    "        logits = self.cls_fc(hidden_state)\r\n",
    "\r\n",
    "        return logits, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 训练配置\r\n",
    "# 配置模型参数（如：训练轮次、学习率等）、训练资源、实例化模型并指定优化器。\r\n",
    "# 学习率是优化器的一个参数，代表参数更新幅度的大小，即步长。当学习率最优时，模型的有效容量最大，最终能达到的效果最好。\r\n",
    "# SGD是比较成熟的优化算法之一，每次训练少量数据，基于这部分数据计算梯度和损失来更新参数。\r\n",
    "\r\n",
    "# 训练配置\r\n",
    "paddle.seed(0)\r\n",
    "np.random.seed(0)\r\n",
    "random.seed(0)\r\n",
    "\r\n",
    "# 设置模型参数\r\n",
    "epochs = 5\r\n",
    "learning_rate = 0.05\r\n",
    "batch_size = 1\r\n",
    "num_steps = 10\r\n",
    "\r\n",
    "vocab_size = 10\r\n",
    "emb_size = 128\r\n",
    "hidden_size = 128\r\n",
    "\r\n",
    "# 指定训练资源\r\n",
    "use_gpu = True if paddle.get_device().startswith(\"gpu\") else False\r\n",
    "if use_gpu:\r\n",
    "    paddle.set_device('gpu:0')\r\n",
    "\r\n",
    "# 实例化模型\r\n",
    "model = NumericPrediction(vocab_size, emb_size, hidden_size, model_type=\"RNN\")\r\n",
    "\r\n",
    "# 指定优化器\r\n",
    "optimizer = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.SelfRNN'>\n",
      "=========epoch: 1, step: 0, loss: 8.67871========\n",
      "W_grad_l2: 19.658676, U_grad_l2: 47.573059, b_grad_l2: 13.377793 \n",
      "=========epoch: 1, step: 1, loss: 8.77148========\n",
      "W_grad_l2: 15.004972, U_grad_l2: 78.789055, b_grad_l2: 9.303792 \n",
      "=========epoch: 1, step: 2, loss: 109.83491========\n",
      "W_grad_l2: 263.725128, U_grad_l2: 948.933655, b_grad_l2: 152.599915 \n",
      "=========epoch: 1, step: 3, loss: 2279.41919========\n",
      "W_grad_l2: 5637.591797, U_grad_l2: 2853.397949, b_grad_l2: 655.646729 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 2, step: 0, loss: 50372.83594========\n",
      "W_grad_l2: 34280.796875, U_grad_l2: 27940.976562, b_grad_l2: 2501.016113 \n",
      "=========epoch: 2, step: 1, loss: 3410761.50000========\n",
      "W_grad_l2: 130793.921875, U_grad_l2: 8154.219727, b_grad_l2: 720.770386 \n",
      "=========epoch: 2, step: 2, loss: 269143584.00000========\n",
      "W_grad_l2: 7621075.500000, U_grad_l2: 427715.187500, b_grad_l2: 37806.281250 \n",
      "=========epoch: 2, step: 3, loss: 17625387008.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 3, step: 0, loss: 2495934627840.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 3, step: 1, loss: 353449468231680.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 3, step: 2, loss: 50051989794455552.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 3, step: 3, loss: 7087862620237070336.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 4, step: 0, loss: 1003712182402481651712.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 4, step: 1, loss: 142135675895641444188160.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 4, step: 2, loss: 20127832754634850473869312.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 4, step: 3, loss: 2850302349898046109856563200.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 5, step: 0, loss: 403631354728585793291360927744.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 5, step: 1, loss: 57158249700840311847426243690496.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 5, step: 2, loss: 8094179958274571451977718652993536.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "=========epoch: 5, step: 3, loss: 1146216803260991308623702457256509440.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:143: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if data.dtype == np.object:\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\r\n",
    "# 在训练过程中，每轮迭代打印一次训练结果，观察Loss和RNN中参数W、U和b的梯度信息。\r\n",
    "# 为了方便观察模型训练效果，计算梯度矩阵的L2范数进行展示，L2范数越大，代表梯度矩阵中的值也越大，越倾向产生梯度爆炸。\r\n",
    "\r\n",
    "from numpy.linalg import norm\r\n",
    "\r\n",
    "def output_grads_l2(model, hidden_state):\r\n",
    "\r\n",
    "    W_grad_l2, U_grad_l2, b_grad_l2 = 0, 0, 0\r\n",
    "    for name, param in model.named_parameters(): \r\n",
    "        if name == \"model.W\":  \r\n",
    "            W_grad_l2 = norm(param.grad)\r\n",
    "        if name == \"model.U\": \r\n",
    "            U_grad_l2 = norm(param.grad)\r\n",
    "        if name == \"model.b\": \r\n",
    "            b_grad_l2 = norm(param.grad)\r\n",
    "\r\n",
    "    return W_grad_l2, U_grad_l2, b_grad_l2\r\n",
    "\r\n",
    "\r\n",
    "# 开始训练\r\n",
    "def train(model, logging_steps=1):\r\n",
    "    model.train()\r\n",
    "    print(type(model.model))\r\n",
    "    global_step = 0\r\n",
    "\r\n",
    "    for epoch in range(1, epochs+1):\r\n",
    "        total_count = 0\r\n",
    "        correct_count = 0\r\n",
    "        for step, batch in enumerate(load_data(num_steps=num_steps)):\r\n",
    "            global_step += 1\r\n",
    "            batch_seq, batch_label = batch\r\n",
    "            predicts, hidden_state = model(batch_seq)\r\n",
    "            loss = F.mse_loss(predicts, batch_label)\r\n",
    "\r\n",
    "            loss.backward()\r\n",
    "\r\n",
    "            if global_step % logging_steps ==0:\r\n",
    "                print(\"=========epoch: %d, step: %d, loss: %.5f========\" % (epoch, step, loss))\r\n",
    "                W_grad_l2, U_grad_l2, b_grad_l2 = output_grads_l2(model, hidden_state)\r\n",
    "                print(\"W_grad_l2: %f, U_grad_l2: %f, b_grad_l2: %f \" % (W_grad_l2, U_grad_l2, b_grad_l2))\r\n",
    "\r\n",
    "            optimizer.step()\r\n",
    "            optimizer.clear_grad()\r\n",
    "\r\n",
    "            with paddle.no_grad():\r\n",
    "                # 检验是否预测正确\r\n",
    "                predicts = predicts.squeeze(1)\r\n",
    "                diff = (predicts - batch_label).abs()\r\n",
    "                correct_count += paddle.cast(diff<0.1, \"int64\").sum().numpy()[0]\r\n",
    "                total_count += len(batch_seq)\r\n",
    "\r\n",
    "        if global_step % logging_steps ==0:\r\n",
    "            acc = correct_count/total_count\r\n",
    "            print(\"\\ncorrect/total:%d/%d, Accuracy: %.5f \\n\" % (correct_count, total_count, acc))\r\n",
    "\r\n",
    "\r\n",
    "train(model, logging_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.SelfRNN'>\n",
      "=========epoch: 5, step: 3, loss: 0.19845========\n",
      "W_grad_l2: 1.107595, U_grad_l2: 4.565491, b_grad_l2: 0.695140 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 10, step: 3, loss: 0.20502========\n",
      "W_grad_l2: 1.322490, U_grad_l2: 4.772387, b_grad_l2: 0.771956 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 15, step: 3, loss: 0.01348========\n",
      "W_grad_l2: 0.813176, U_grad_l2: 2.182861, b_grad_l2: 0.439725 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 20, step: 3, loss: 0.00554========\n",
      "W_grad_l2: 0.733498, U_grad_l2: 1.932203, b_grad_l2: 0.363847 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 25, step: 3, loss: 0.98179========\n",
      "W_grad_l2: 4.601934, U_grad_l2: 14.251498, b_grad_l2: 2.300374 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 30, step: 3, loss: 0.07460========\n",
      "W_grad_l2: 1.674462, U_grad_l2: 4.972064, b_grad_l2: 0.842681 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 35, step: 3, loss: 0.02793========\n",
      "W_grad_l2: 0.791536, U_grad_l2: 2.609462, b_grad_l2: 0.438870 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 40, step: 3, loss: 0.30650========\n",
      "W_grad_l2: 2.804833, U_grad_l2: 9.227398, b_grad_l2: 1.507882 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 45, step: 3, loss: 0.84398========\n",
      "W_grad_l2: 3.689042, U_grad_l2: 12.382934, b_grad_l2: 2.011355 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 50, step: 3, loss: 0.00158========\n",
      "W_grad_l2: 0.147262, U_grad_l2: 0.521812, b_grad_l2: 0.081364 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 55, step: 3, loss: 0.05959========\n",
      "W_grad_l2: 1.063191, U_grad_l2: 3.671713, b_grad_l2: 0.617613 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 60, step: 3, loss: 0.73016========\n",
      "W_grad_l2: 3.513190, U_grad_l2: 12.598075, b_grad_l2: 2.098394 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 65, step: 3, loss: 0.03224========\n",
      "W_grad_l2: 0.647461, U_grad_l2: 2.426347, b_grad_l2: 0.412468 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 70, step: 3, loss: 0.02031========\n",
      "W_grad_l2: 0.506253, U_grad_l2: 1.917277, b_grad_l2: 0.296418 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 75, step: 3, loss: 0.29923========\n",
      "W_grad_l2: 1.868638, U_grad_l2: 7.159494, b_grad_l2: 1.145701 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 80, step: 3, loss: 0.02648========\n",
      "W_grad_l2: 0.544686, U_grad_l2: 2.154880, b_grad_l2: 0.351692 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 85, step: 3, loss: 0.16206========\n",
      "W_grad_l2: 1.185090, U_grad_l2: 4.881819, b_grad_l2: 0.774823 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 90, step: 3, loss: 0.09425========\n",
      "W_grad_l2: 0.929307, U_grad_l2: 3.872819, b_grad_l2: 0.575172 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 95, step: 3, loss: 0.34187========\n",
      "W_grad_l2: 1.518758, U_grad_l2: 6.349827, b_grad_l2: 0.994450 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 100, step: 3, loss: 0.01486========\n",
      "W_grad_l2: 0.339483, U_grad_l2: 1.418469, b_grad_l2: 0.221474 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 105, step: 3, loss: 0.08626========\n",
      "W_grad_l2: 0.631880, U_grad_l2: 2.721009, b_grad_l2: 0.403983 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 110, step: 3, loss: 0.00029========\n",
      "W_grad_l2: 0.045008, U_grad_l2: 0.188548, b_grad_l2: 0.026817 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 115, step: 3, loss: 0.00260========\n",
      "W_grad_l2: 0.149375, U_grad_l2: 0.615120, b_grad_l2: 0.093784 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 120, step: 3, loss: 0.11933========\n",
      "W_grad_l2: 0.898705, U_grad_l2: 3.887277, b_grad_l2: 0.564577 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 125, step: 3, loss: 0.53082========\n",
      "W_grad_l2: 1.607147, U_grad_l2: 7.074773, b_grad_l2: 1.037398 \n",
      "\n",
      "correct/total:3/4, Accuracy: 0.75000 \n",
      "\n",
      "=========epoch: 130, step: 3, loss: 0.08261========\n",
      "W_grad_l2: 0.695744, U_grad_l2: 3.147629, b_grad_l2: 0.454213 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 135, step: 3, loss: 0.64592========\n",
      "W_grad_l2: 1.640720, U_grad_l2: 7.564490, b_grad_l2: 1.100180 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 140, step: 3, loss: 0.04140========\n",
      "W_grad_l2: 0.467577, U_grad_l2: 2.182169, b_grad_l2: 0.309338 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 145, step: 3, loss: 0.47754========\n",
      "W_grad_l2: 1.364028, U_grad_l2: 6.505651, b_grad_l2: 0.925623 \n",
      "\n",
      "correct/total:3/4, Accuracy: 0.75000 \n",
      "\n",
      "=========epoch: 150, step: 3, loss: 0.44404========\n",
      "W_grad_l2: 1.600953, U_grad_l2: 7.535597, b_grad_l2: 1.094857 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 155, step: 3, loss: 0.55647========\n",
      "W_grad_l2: 1.705641, U_grad_l2: 8.129462, b_grad_l2: 1.171998 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 160, step: 3, loss: 0.20749========\n",
      "W_grad_l2: 0.879259, U_grad_l2: 4.198201, b_grad_l2: 0.594927 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 165, step: 3, loss: 0.00231========\n",
      "W_grad_l2: 0.100933, U_grad_l2: 0.482976, b_grad_l2: 0.067442 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 170, step: 3, loss: 0.00161========\n",
      "W_grad_l2: 0.089749, U_grad_l2: 0.421733, b_grad_l2: 0.058232 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 175, step: 3, loss: 0.14552========\n",
      "W_grad_l2: 0.818081, U_grad_l2: 3.971421, b_grad_l2: 0.531766 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 180, step: 3, loss: 0.08780========\n",
      "W_grad_l2: 0.542636, U_grad_l2: 2.612396, b_grad_l2: 0.366028 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 185, step: 3, loss: 0.19048========\n",
      "W_grad_l2: 0.803190, U_grad_l2: 3.944978, b_grad_l2: 0.541206 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 190, step: 3, loss: 0.36466========\n",
      "W_grad_l2: 1.012326, U_grad_l2: 4.847543, b_grad_l2: 0.684573 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 195, step: 3, loss: 0.07871========\n",
      "W_grad_l2: 0.493328, U_grad_l2: 2.442386, b_grad_l2: 0.332792 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 200, step: 3, loss: 0.46396========\n",
      "W_grad_l2: 1.083841, U_grad_l2: 5.292490, b_grad_l2: 0.727635 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 205, step: 3, loss: 0.04759========\n",
      "W_grad_l2: 0.364427, U_grad_l2: 1.835659, b_grad_l2: 0.243955 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 210, step: 3, loss: 0.49818========\n",
      "W_grad_l2: 1.047215, U_grad_l2: 5.259634, b_grad_l2: 0.707848 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 215, step: 3, loss: 0.01346========\n",
      "W_grad_l2: 0.186618, U_grad_l2: 0.959940, b_grad_l2: 0.125024 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 220, step: 3, loss: 0.44096========\n",
      "W_grad_l2: 0.966568, U_grad_l2: 4.981393, b_grad_l2: 0.664435 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 225, step: 3, loss: 0.04382========\n",
      "W_grad_l2: 0.339539, U_grad_l2: 1.759413, b_grad_l2: 0.229805 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 230, step: 3, loss: 0.50630========\n",
      "W_grad_l2: 0.947907, U_grad_l2: 5.027363, b_grad_l2: 0.656342 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 235, step: 3, loss: 0.01903========\n",
      "W_grad_l2: 0.200082, U_grad_l2: 1.072255, b_grad_l2: 0.134962 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 240, step: 3, loss: 0.52340========\n",
      "W_grad_l2: 0.896561, U_grad_l2: 4.874312, b_grad_l2: 0.621869 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 245, step: 3, loss: 0.01519========\n",
      "W_grad_l2: 0.168547, U_grad_l2: 0.916229, b_grad_l2: 0.112808 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 250, step: 3, loss: 0.51193========\n",
      "W_grad_l2: 0.835822, U_grad_l2: 4.632072, b_grad_l2: 0.576424 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 255, step: 3, loss: 0.01543========\n",
      "W_grad_l2: 0.165064, U_grad_l2: 0.903941, b_grad_l2: 0.108837 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 260, step: 3, loss: 0.51511========\n",
      "W_grad_l2: 0.797686, U_grad_l2: 4.503292, b_grad_l2: 0.544728 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 265, step: 3, loss: 0.01563========\n",
      "W_grad_l2: 0.165217, U_grad_l2: 0.910197, b_grad_l2: 0.107133 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 270, step: 3, loss: 0.52982========\n",
      "W_grad_l2: 0.783997, U_grad_l2: 4.499813, b_grad_l2: 0.527327 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 275, step: 3, loss: 0.01848========\n",
      "W_grad_l2: 0.187531, U_grad_l2: 1.029317, b_grad_l2: 0.118885 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 280, step: 3, loss: 0.51152========\n",
      "W_grad_l2: 0.801623, U_grad_l2: 4.570275, b_grad_l2: 0.523959 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 285, step: 3, loss: 0.17550========\n",
      "W_grad_l2: 0.714682, U_grad_l2: 3.763698, b_grad_l2: 0.445440 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 290, step: 3, loss: 0.41516========\n",
      "W_grad_l2: 0.764567, U_grad_l2: 4.279461, b_grad_l2: 0.494489 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 295, step: 3, loss: 0.03024========\n",
      "W_grad_l2: 0.268787, U_grad_l2: 1.450855, b_grad_l2: 0.169364 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 300, step: 3, loss: 0.06408========\n",
      "W_grad_l2: 0.451860, U_grad_l2: 2.351220, b_grad_l2: 0.280160 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 305, step: 3, loss: 0.02425========\n",
      "W_grad_l2: 0.245407, U_grad_l2: 1.293583, b_grad_l2: 0.155210 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 310, step: 3, loss: 0.04310========\n",
      "W_grad_l2: 0.373691, U_grad_l2: 1.919223, b_grad_l2: 0.237252 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 315, step: 3, loss: 0.02113========\n",
      "W_grad_l2: 0.170196, U_grad_l2: 0.913025, b_grad_l2: 0.110944 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 320, step: 3, loss: 0.09634========\n",
      "W_grad_l2: 0.448701, U_grad_l2: 2.232461, b_grad_l2: 0.282610 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 325, step: 3, loss: 0.30115========\n",
      "W_grad_l2: 0.929320, U_grad_l2: 4.649379, b_grad_l2: 0.558568 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 330, step: 3, loss: 0.23743========\n",
      "W_grad_l2: 0.695304, U_grad_l2: 3.407507, b_grad_l2: 0.427000 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 335, step: 3, loss: 0.07570========\n",
      "W_grad_l2: 0.511857, U_grad_l2: 2.500422, b_grad_l2: 0.300575 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 340, step: 3, loss: 0.08465========\n",
      "W_grad_l2: 0.416331, U_grad_l2: 2.145760, b_grad_l2: 0.256957 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 345, step: 3, loss: 0.17356========\n",
      "W_grad_l2: 0.712537, U_grad_l2: 3.608189, b_grad_l2: 0.424850 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 350, step: 3, loss: 0.00007========\n",
      "W_grad_l2: 0.013411, U_grad_l2: 0.070214, b_grad_l2: 0.008210 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 355, step: 3, loss: 0.09797========\n",
      "W_grad_l2: 0.527837, U_grad_l2: 2.793436, b_grad_l2: 0.317503 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 360, step: 3, loss: 0.23036========\n",
      "W_grad_l2: 0.616469, U_grad_l2: 3.167303, b_grad_l2: 0.378176 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 365, step: 3, loss: 0.02000========\n",
      "W_grad_l2: 0.261798, U_grad_l2: 1.248797, b_grad_l2: 0.149274 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 370, step: 3, loss: 0.02068========\n",
      "W_grad_l2: 0.188459, U_grad_l2: 1.006658, b_grad_l2: 0.117199 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 375, step: 3, loss: 0.06377========\n",
      "W_grad_l2: 0.360605, U_grad_l2: 1.813189, b_grad_l2: 0.209750 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 380, step: 3, loss: 0.09335========\n",
      "W_grad_l2: 0.462149, U_grad_l2: 2.401312, b_grad_l2: 0.273781 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 385, step: 3, loss: 0.25000========\n",
      "W_grad_l2: 0.657279, U_grad_l2: 3.190867, b_grad_l2: 0.387155 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 390, step: 3, loss: 0.00246========\n",
      "W_grad_l2: 0.065749, U_grad_l2: 0.329453, b_grad_l2: 0.039701 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 395, step: 3, loss: 0.16325========\n",
      "W_grad_l2: 0.655972, U_grad_l2: 3.113196, b_grad_l2: 0.359716 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 400, step: 3, loss: 0.00624========\n",
      "W_grad_l2: 0.125766, U_grad_l2: 0.610402, b_grad_l2: 0.071964 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 405, step: 3, loss: 0.09007========\n",
      "W_grad_l2: 0.507796, U_grad_l2: 2.343419, b_grad_l2: 0.268105 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 410, step: 3, loss: 0.17799========\n",
      "W_grad_l2: 0.520789, U_grad_l2: 2.657384, b_grad_l2: 0.293027 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 415, step: 3, loss: 0.05893========\n",
      "W_grad_l2: 0.347571, U_grad_l2: 1.851295, b_grad_l2: 0.199879 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 420, step: 3, loss: 0.00338========\n",
      "W_grad_l2: 0.068693, U_grad_l2: 0.341432, b_grad_l2: 0.039724 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 425, step: 3, loss: 0.04431========\n",
      "W_grad_l2: 0.388442, U_grad_l2: 1.747819, b_grad_l2: 0.205613 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 430, step: 3, loss: 0.28663========\n",
      "W_grad_l2: 0.626648, U_grad_l2: 3.205754, b_grad_l2: 0.348461 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 435, step: 3, loss: 0.03615========\n",
      "W_grad_l2: 0.276068, U_grad_l2: 1.445943, b_grad_l2: 0.156540 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 440, step: 3, loss: 0.30579========\n",
      "W_grad_l2: 0.647931, U_grad_l2: 3.250813, b_grad_l2: 0.376080 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 445, step: 3, loss: 0.04935========\n",
      "W_grad_l2: 0.365942, U_grad_l2: 1.715000, b_grad_l2: 0.201842 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 450, step: 3, loss: 0.35360========\n",
      "W_grad_l2: 0.753047, U_grad_l2: 3.674278, b_grad_l2: 0.436872 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 455, step: 3, loss: 0.03278========\n",
      "W_grad_l2: 0.297860, U_grad_l2: 1.402454, b_grad_l2: 0.166498 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 460, step: 3, loss: 0.37029========\n",
      "W_grad_l2: 0.785198, U_grad_l2: 3.869207, b_grad_l2: 0.456817 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 465, step: 3, loss: 0.00892========\n",
      "W_grad_l2: 0.173542, U_grad_l2: 0.854824, b_grad_l2: 0.099138 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 470, step: 3, loss: 0.24834========\n",
      "W_grad_l2: 0.592632, U_grad_l2: 3.076426, b_grad_l2: 0.342660 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 475, step: 3, loss: 0.00153========\n",
      "W_grad_l2: 0.072760, U_grad_l2: 0.369340, b_grad_l2: 0.040374 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 480, step: 3, loss: 0.02778========\n",
      "W_grad_l2: 0.195772, U_grad_l2: 1.011202, b_grad_l2: 0.112522 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 485, step: 3, loss: 0.00056========\n",
      "W_grad_l2: 0.043205, U_grad_l2: 0.196291, b_grad_l2: 0.022902 \n",
      "\n",
      "correct/total:2/4, Accuracy: 0.50000 \n",
      "\n",
      "=========epoch: 490, step: 3, loss: 0.01862========\n",
      "W_grad_l2: 0.249301, U_grad_l2: 1.208564, b_grad_l2: 0.132121 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 495, step: 3, loss: 0.18272========\n",
      "W_grad_l2: 0.541880, U_grad_l2: 2.732810, b_grad_l2: 0.308746 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 500, step: 3, loss: 0.05388========\n",
      "W_grad_l2: 0.357207, U_grad_l2: 1.854936, b_grad_l2: 0.202235 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RNN网络优化：梯度截断\r\n",
    "# 针对RNN梯度爆炸的情况，可以采用梯度截断的方式缓解，当梯度达到一定阈值时，对其进行截断。\r\n",
    "# 一般截断有两种方式：按值截断和按模截断。\r\n",
    "# 本实验采用按模截断的方式，使用 ClipGradByGlobalNorm API。在代码实现时，将ClipGradByNorm传入优化器，优化器在反向迭代过程中，每次梯度更新时便可以梯度裁剪。\r\n",
    "\r\n",
    "# 训练配置\r\n",
    "paddle.seed(0)\r\n",
    "np.random.seed(0)\r\n",
    "random.seed(0)\r\n",
    "\r\n",
    "# 设置模型参数\r\n",
    "epochs = 500\r\n",
    "learning_rate = 0.05\r\n",
    "batch_size = 1\r\n",
    "num_steps = 10\r\n",
    "\r\n",
    "vocab_size = 10\r\n",
    "emb_size = 128\r\n",
    "hidden_size = 128\r\n",
    "\r\n",
    "# 指定训练资源\r\n",
    "use_gpu = True if paddle.get_device().startswith(\"gpu\") else False\r\n",
    "if use_gpu:\r\n",
    "    paddle.set_device('gpu:0')\r\n",
    "\r\n",
    "# 实例化模型\r\n",
    "model = NumericPrediction(vocab_size, emb_size, hidden_size, model_type=\"RNN\")\r\n",
    "\r\n",
    "# 指定优化器\r\n",
    "clip = paddle.nn.ClipGradByNorm(clip_norm=1.0)\r\n",
    "optimizer = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=model.parameters(), weight_decay=0.,grad_clip=clip)\r\n",
    "\r\n",
    "#训练模型\r\n",
    "train(model, logging_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LSTM网络搭建\r\n",
    "# LSTM网络的代码实现与RNN结构相似，只是在RNN的基础上增加了隐藏门、输入门、遗忘门的定义和计算。这里依然选择保留序列的最后一个单词位置的输出向量。\r\n",
    "\r\n",
    "# 导入paddle\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "\r\n",
    "# 声明LSTM网络和相关参数\r\n",
    "class SelfLSTM(paddle.nn.Layer):\r\n",
    "    def __init__(self, emb_dim, hidden_size):\r\n",
    "        super(SelfLSTM, self).__init__()\r\n",
    "        self.emb_dim = emb_dim\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.w_i = paddle.create_parameter(shape=[emb_dim, hidden_size], dtype=\"float32\")\r\n",
    "        self.w_f = paddle.create_parameter(shape=[emb_dim, hidden_size], dtype=\"float32\")\r\n",
    "        self.w_o = paddle.create_parameter(shape=[emb_dim, hidden_size], dtype=\"float32\")\r\n",
    "        self.w_a = paddle.create_parameter(shape=[emb_dim, hidden_size], dtype=\"float32\")\r\n",
    "        self.u_i = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\")\r\n",
    "        self.u_f = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\")\r\n",
    "        self.u_o = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\")\r\n",
    "        self.u_a = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=\"float32\")\r\n",
    "        self.b_i = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\")\r\n",
    "        self.b_f = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\")\r\n",
    "        self.b_o = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\")\r\n",
    "        self.b_a = paddle.create_parameter(shape=[1, hidden_size], dtype=\"float32\")\r\n",
    "\r\n",
    "    # 定义前向计算\r\n",
    "    def forward(self, inputs):\r\n",
    "        # inputs batch_size x seq_len x emb_dim\r\n",
    "        batch_size, seq_len, emb_dim = inputs.shape\r\n",
    "\r\n",
    "        # 初始化状态向量和隐状态向量\r\n",
    "        cell_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=\"float32\")\r\n",
    "        hidden_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=\"float32\")\r\n",
    "\r\n",
    "        # 执行LSTM计算，包括：隐藏门、输入门、遗忘门、候选状态向量、状态向量和隐状态向量\r\n",
    "        for step in range(seq_len):\r\n",
    "            input_step = inputs[:, step, :]\r\n",
    "            i = F.sigmoid(paddle.matmul(input_step, self.w_i) + paddle.matmul(hidden_state, self.u_i) + self.b_i)\r\n",
    "            f = F.sigmoid(paddle.matmul(input_step, self.w_f) + paddle.matmul(hidden_state, self.u_f) + self.b_f)\r\n",
    "            o = F.sigmoid(paddle.matmul(input_step, self.w_o) + paddle.matmul(hidden_state, self.u_o) + self.b_o)\r\n",
    "            c_tilde = F.tanh(paddle.matmul(input_step, self.w_a) + paddle.matmul(hidden_state, self.u_a) + self.b_a)\r\n",
    "            cell_state = f * cell_state + i * c_tilde\r\n",
    "            hidden_state = o * F.tanh(cell_state)\r\n",
    "\r\n",
    "        return cell_state, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.SelfLSTM'>\n",
      "=========epoch: 20, step: 3, loss: 1.01558========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 40, step: 3, loss: 0.88890========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 60, step: 3, loss: 0.77657========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 80, step: 3, loss: 0.70785========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 100, step: 3, loss: 0.40821========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 120, step: 3, loss: 1.24923========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 140, step: 3, loss: 0.63072========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 160, step: 3, loss: 0.54075========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 180, step: 3, loss: 0.14167========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:0/4, Accuracy: 0.00000 \n",
      "\n",
      "=========epoch: 200, step: 3, loss: 0.01460========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:1/4, Accuracy: 0.25000 \n",
      "\n",
      "=========epoch: 220, step: 3, loss: 0.00426========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 240, step: 3, loss: 0.00007========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 260, step: 3, loss: 0.00226========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 280, step: 3, loss: 0.00329========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 300, step: 3, loss: 0.00097========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 320, step: 3, loss: 0.00001========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 340, step: 3, loss: 0.00012========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 360, step: 3, loss: 0.00015========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 380, step: 3, loss: 0.00005========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 400, step: 3, loss: 0.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 420, step: 3, loss: 0.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 440, step: 3, loss: 0.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 460, step: 3, loss: 0.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 480, step: 3, loss: 0.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n",
      "=========epoch: 500, step: 3, loss: 0.00000========\n",
      "W_grad_l2: 0.000000, U_grad_l2: 0.000000, b_grad_l2: 0.000000 \n",
      "\n",
      "correct/total:4/4, Accuracy: 1.00000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 基于LSTM网络进行数字预测\r\n",
    "# 实验过程中尽量复用RNN的参数配置，模型实例化方面需要将数字预测模型NumericPrediction中的model_type设置为LSTM，进行模型训练和评估。\r\n",
    "# 在训练过程中统计模型预测的准确率，假如模型预测的数值和原本标签数据的差值在[-0.1,0.1]之内，则认为预测正确，否则认为预测错误。\r\n",
    "\r\n",
    "# 训练配置\r\n",
    "paddle.seed(0)\r\n",
    "np.random.seed(0)\r\n",
    "random.seed(0)\r\n",
    "\r\n",
    "# 设置模型参数\r\n",
    "epochs = 500\r\n",
    "learning_rate = 0.1\r\n",
    "batch_size = 1\r\n",
    "num_steps = 10\r\n",
    "\r\n",
    "vocab_size = 10\r\n",
    "emb_size = 128\r\n",
    "hidden_size = 128\r\n",
    "\r\n",
    "# 指定训练资源\r\n",
    "use_gpu = True if paddle.get_device().startswith(\"gpu\") else False\r\n",
    "if use_gpu:\r\n",
    "    paddle.set_device('gpu:0')\r\n",
    "\r\n",
    "# 实例化模型\r\n",
    "model = NumericPrediction(vocab_size, emb_size, hidden_size, model_type=\"LSTM\")\r\n",
    "\r\n",
    "# 指定优化器\r\n",
    "clip = paddle.nn.ClipGradByNorm(clip_norm=1.0)\r\n",
    "optimizer = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=model.parameters(), grad_clip=clip)\r\n",
    "\r\n",
    "train(model, logging_steps=80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
