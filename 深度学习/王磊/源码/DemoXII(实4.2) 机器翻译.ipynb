{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **任务介绍**\n",
    "\n",
    "机器翻译：即用计算机实现从源语言到目标语言转换的过程，是自然语言处理的重要研究领域之一。\n",
    "\n",
    "源语言：被翻译的语言\n",
    "\n",
    "目标语言：翻译后的结果语言\n",
    "\n",
    "\n",
    "# **数据集介绍**\n",
    "\n",
    "**数据集：WMT-14数据集**\n",
    "\n",
    "    该数据集有193319条训练数据，6003条测试数据，词典长度为30000。\n",
    "\n",
    "    Paddle接口paddle.dataset.wmt14中默认提供了一个经过预处理的较小规模的数据集。\n",
    "\n",
    "数据预处理：\n",
    "\n",
    "    将每个源语言到目标语言的平行语料库文件合并为一个文件，合并每个xxx.src和xxx.trg文件为xxx；xxx中的第i行内容为xxx.src的第i行和xxx.trg中的第i行连接，用“t”分隔。\n",
    "\n",
    "    创建训练数据的源字典和目标字典。每个字典都有DICSIZE个单词，包括语料中词频最高的DICSIZE-3个单词和三个特殊符号：\n",
    "\n",
    "    < s >表示序列的开始\n",
    "    < e >表示序列的结束\n",
    "    < unk >表示未登录词\n",
    "\n",
    "# **实践流程**\n",
    "# **1、准备数据**\n",
    "# **2、配置网络**\n",
    "\n",
    "    定义网络\n",
    "    定义损失函数\n",
    "    定义优化算法\n",
    "\n",
    "# **3、训练网络**\n",
    "# **4、模型预测**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#导入需要的包\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import paddle as paddle\n",
    "\n",
    "import paddle.fluid as fluid\n",
    "\n",
    "import os\n",
    "\n",
    "import paddle.fluid.layers as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_size = 30000 #字典维度\n",
    "\n",
    "source_dict_dim = target_dict_dim = dict_size # source_dict_dim:源语言字典维度 target_dict_dim：目标语言字典维度\n",
    "\n",
    "hidden_dim = 32 #解码中隐层大小\n",
    "\n",
    "decoder_size = hidden_dim #解码中隐层大小\n",
    "\n",
    "word_dim = 32 #词向量维度\n",
    "\n",
    "batch_size = 200 #数据提供器每次读入的数据批次大小\n",
    "\n",
    "max_length = 8 #生成句子的最大长度\n",
    "\n",
    "beam_size = 2 #柱宽度\n",
    "\n",
    "is_sparse = True #代表是否用稀疏更新的标志\n",
    "\n",
    "model_save_dir = \"machine_translation.inference.model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **1、准备数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# *********获取训练数据读取器和测试数据读取器train_reader 和test_reader***************\n",
    "\n",
    "train_reader = paddle.batch(\n",
    "\n",
    "        paddle.reader.shuffle(\n",
    "\n",
    "            paddle.dataset.wmt14.train(dict_size),buf_size=1000),#dict_size:字典维度 buf_size:乱序时的缓存大小\n",
    "\n",
    "        batch_size=batch_size)                                   #batch_size:批次数据大小\n",
    "\n",
    "#加载预测的数据\n",
    "\n",
    "test_reader = paddle.batch(\n",
    "\n",
    "    paddle.reader.shuffle(\n",
    "\n",
    "        paddle.dataset.wmt14.test(dict_size), buf_size=1000),\n",
    "\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **编码器解码器框架**\n",
    "\n",
    "解决的问题：由任意一个长度的原序列到另一个长度的目标序列的变化问题\n",
    "\n",
    "    编码：将整个原序列表征成一个向量\n",
    "\n",
    "    解码：通过最大化预测序列概率，从中解码出整个目标序列\n",
    "\n",
    "**柱搜索算法**\n",
    "\n",
    "    启发式搜索算法：在图或树中搜索每一步的最优扩展节点\n",
    "    \n",
    "    贪心算法：每一步最优，全局不一定最优\n",
    "    \n",
    "    场景：解空间非常大，内存装不下所有展开解的系统\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/5586f825a40e4a14a0fe9b9bd10d98e5f74d15d98c9d420b8921c8061b1c898a)\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **2、配置网络**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#************************实现编码器*******************************\n",
    "\n",
    "def encoder():\n",
    "\n",
    "    #输入是一个文字序列，被表示成整型的序列。\n",
    "    #lod_level=1 if the input is sequential data, and otherwise lod_level=0.\n",
    "\n",
    "    src_word_id = pd.data(\n",
    "\n",
    "        name=\"src_word_id\", shape=[1], dtype='int64', lod_level=1)\n",
    "\n",
    "    #将上述编码映射到低维语言空间的词向量\n",
    "\n",
    "    src_embedding = pd.embedding(\n",
    "\n",
    "        input=src_word_id,         #输入为独热编码\n",
    "\n",
    "        size=[dict_size, word_dim],#dict_size:字典维度 word_dim：词向量维度\n",
    "\n",
    "        dtype='float32',           \n",
    "\n",
    "        is_sparse=is_sparse,       #代表是否用稀疏更新的标志\n",
    "\n",
    "        #ParamAttr类代表了参数的各种属性。\n",
    "        #比如learning rate（学习率）, regularization（正则化）, trainable（可训练性）, do_model_average(平均化模型)和参数初始化方法.\n",
    "        \n",
    "        param_attr=fluid.ParamAttr(name='vemb'))   #可学习的隐藏层权重的参数属性。\n",
    "\n",
    "\n",
    "\n",
    "    #输入门、遗忘门和输出门中对于输入词向量的全连接操作 (Wfx, Wix, Wox, Wcx)\n",
    "    #这些操作不包括在dynamic_lstm运算中，用户在LSTM operator之前选择全连接运算\n",
    "    \n",
    "    fc1 = pd.fc(input=src_embedding, size=hidden_dim * 4, act='tanh')\n",
    "\n",
    "    #初始化lstm网络，注意size=hidden_dim*4\n",
    "    #返回：隐藏状态(hidden state)，LSTM神经元状态，两者都是T*D维\n",
    "    \n",
    "    lstm_hidden0, lstm_0 = pd.dynamic_lstm(input=fc1, size=hidden_dim * 4)\n",
    "\n",
    "    #完成所有时间步内的lstm计算，得到编码的最终输出\n",
    "\n",
    "    encoder_out = pd.sequence_last_step(input=lstm_hidden0)\n",
    "\n",
    "    return encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#************************定义训练模式下的解码器*******************************\n",
    "\n",
    "def train_decoder(context):\n",
    "\n",
    "    #获取目标语言序列\n",
    "\n",
    "    trg_language_word = pd.data(\n",
    "\n",
    "        name=\"target_language_word\", shape=[1], dtype='int64', lod_level=1)\n",
    "\n",
    "    #获取目标语言的词向量\n",
    "\n",
    "    trg_embedding = pd.embedding(\n",
    "\n",
    "        input=trg_language_word,\n",
    "\n",
    "        size=[dict_size, word_dim],#dict_size:字典维度 word_dim：词向量维度\n",
    "\n",
    "        dtype='float32',\n",
    "\n",
    "        is_sparse=is_sparse,\n",
    "\n",
    "        param_attr=fluid.ParamAttr(name='vemb'))\n",
    "\n",
    "    rnn = pd.DynamicRNN()\n",
    "\n",
    "    #定义RNN每一步的运算\n",
    "    with rnn.block():\n",
    "\n",
    "        #current_word不是序列数据，lod_level=0; trg_embedding是序列数据，lod_level=1\n",
    "        current_word = rnn.step_input(trg_embedding)#current_word:当前节点的输入\n",
    "\n",
    "        pre_state = rnn.memory(init=context, need_reorder=True)#pre_state:上个节点的输出\n",
    "\n",
    "        current_state = pd.fc(\n",
    "\n",
    "            input=[current_word, pre_state], size=decoder_size, act='tanh')#得到当前节点的输出\n",
    "\n",
    "        #对可能输出的单词进行打分，再用softmax函数进行归一化得到当前节点的概率\n",
    "\n",
    "        current_score = pd.fc(input=current_state, size=target_dict_dim, act='softmax')\n",
    "\n",
    "        #更新当前节点的输出为上个节点的输出\n",
    "\n",
    "        rnn.update_memory(pre_state, current_state)\n",
    "\n",
    "        rnn.output(current_score)\n",
    "\n",
    "    return rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paddle.enable_static()\n",
    "\n",
    "#得到编码器\n",
    "\n",
    "context = encoder()\n",
    "\n",
    "#得到解码器\n",
    "\n",
    "rnn_out = train_decoder(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#训练标签\n",
    "\n",
    "label = pd.data(name=\"target_language_next_word\", shape=[1], dtype='int64', lod_level=1)\n",
    "\n",
    "# 用交叉熵损失函数计算损失，并使用mean算子进行损失规约\n",
    "\n",
    "cost = pd.cross_entropy(input=rnn_out, label=label)\n",
    "\n",
    "avg_cost = pd.mean(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#定义优化方法\n",
    "\n",
    "optimizer= fluid.optimizer.Adagrad(\n",
    "\n",
    "        learning_rate=1e-4,\n",
    "\n",
    "        regularization=fluid.regularizer.L2DecayRegularizer(#正则化函数，L2正则化通过限制参数数值\n",
    "                                                            #来缓解过拟合\n",
    "\n",
    "            regularization_coeff=0.1))\n",
    "\n",
    "opts = optimizer.minimize(avg_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0113 13:07:43.145285   892 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.2\n",
      "W0113 13:07:43.150090   892 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#*************创建Executor*********************\n",
    "\n",
    "# 配置运算场所，定义使用CPU还是GPU，使用CPU时use_cuda = False，使用GPU时use_cuda = True\n",
    "\n",
    "use_cuda = True  # 设置为True以使用CUDA\n",
    "place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()  # 使用GPU设备0\n",
    "\n",
    "exe = fluid.Executor(place)\n",
    "\n",
    "# 进行参数初始化\n",
    "\n",
    "exe.run(fluid.default_startup_program())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#定义数据映射器，feed_list对应数据集里每行的三条数据\n",
    "\n",
    "feeder = fluid.DataFeeder( place=place,feed_list=[ 'src_word_id', \n",
    "\n",
    "                                                   'target_language_word', \n",
    "\n",
    "                                                   'target_language_next_word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **3、训练网络**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass:0, Batch:0, Cost:10.30898, Time:3\n",
      "Pass:0, Batch:10, Cost:10.30841, Time:12\n",
      "Pass:0, Batch:20, Cost:10.30821, Time:22\n",
      "Pass:0, Batch:30, Cost:10.30803, Time:32\n",
      "Pass:0, Batch:40, Cost:10.30794, Time:40\n",
      "Pass:0, Batch:50, Cost:10.30794, Time:51\n",
      "Pass:0, Batch:60, Cost:10.30781, Time:60\n",
      "Pass:0, Batch:70, Cost:10.30770, Time:70\n",
      "Pass:0, Batch:80, Cost:10.30762, Time:80\n",
      "Pass:0, Batch:90, Cost:10.30754, Time:90\n",
      "Pass:0, Batch:100, Cost:10.30741, Time:99\n",
      "Pass:0, Batch:110, Cost:10.30729, Time:109\n",
      "Pass:0, Batch:120, Cost:10.30731, Time:118\n",
      "Pass:0, Batch:130, Cost:10.30720, Time:128\n",
      "Pass:0, Batch:140, Cost:10.30709, Time:138\n",
      "Pass:0, Batch:150, Cost:10.30709, Time:147\n",
      "Pass:0, Batch:160, Cost:10.30697, Time:156\n",
      "Pass:0, Batch:170, Cost:10.30695, Time:166\n",
      "Pass:0, Batch:180, Cost:10.30691, Time:175\n",
      "Pass:0, Batch:190, Cost:10.30679, Time:185\n",
      "Pass:0, Batch:200, Cost:10.30671, Time:195\n",
      "Pass:0, Batch:210, Cost:10.30671, Time:205\n",
      "Pass:0, Batch:220, Cost:10.30665, Time:214\n",
      "Pass:0, Batch:230, Cost:10.30655, Time:224\n",
      "Pass:0, Batch:240, Cost:10.30653, Time:234\n",
      "Pass:0, Batch:250, Cost:10.30650, Time:243\n",
      "Pass:0, Batch:260, Cost:10.30642, Time:252\n",
      "Pass:0, Batch:270, Cost:10.30634, Time:262\n",
      "Pass:0, Batch:280, Cost:10.30625, Time:272\n",
      "Pass:0, Batch:290, Cost:10.30617, Time:281\n",
      "Pass:0, Batch:300, Cost:10.30614, Time:291\n",
      "Pass:0, Batch:310, Cost:10.30616, Time:300\n",
      "Pass:0, Batch:320, Cost:10.30609, Time:310\n",
      "Pass:0, Batch:330, Cost:10.30603, Time:319\n",
      "Pass:0, Batch:340, Cost:10.30598, Time:329\n",
      "Pass:0, Batch:350, Cost:10.30592, Time:338\n",
      "Pass:0, Batch:360, Cost:10.30591, Time:348\n",
      "Pass:0, Batch:370, Cost:10.30575, Time:357\n",
      "Pass:0, Batch:380, Cost:10.30576, Time:367\n",
      "Pass:0, Batch:390, Cost:10.30571, Time:377\n",
      "Pass:0, Batch:400, Cost:10.30560, Time:387\n",
      "Pass:0, Batch:410, Cost:10.30560, Time:396\n",
      "Pass:0, Batch:420, Cost:10.30557, Time:405\n",
      "Pass:0, Batch:430, Cost:10.30537, Time:415\n",
      "Pass:0, Batch:440, Cost:10.30547, Time:425\n",
      "Pass:0, Batch:450, Cost:10.30537, Time:435\n",
      "Pass:0, Batch:460, Cost:10.30533, Time:444\n",
      "Pass:0, Batch:470, Cost:10.30531, Time:454\n",
      "Pass:0, Batch:480, Cost:10.30524, Time:463\n",
      "Pass:0, Batch:490, Cost:10.30514, Time:472\n",
      "Pass:0, Batch:500, Cost:10.30515, Time:482\n",
      "Pass:0, Batch:510, Cost:10.30502, Time:491\n",
      "Pass:0, Batch:520, Cost:10.30498, Time:501\n",
      "Pass:0, Batch:530, Cost:10.30502, Time:511\n",
      "Pass:0, Batch:540, Cost:10.30498, Time:520\n",
      "Pass:0, Batch:550, Cost:10.30483, Time:529\n",
      "Pass:0, Batch:560, Cost:10.30475, Time:539\n",
      "Pass:0, Batch:570, Cost:10.30476, Time:549\n",
      "Pass:0, Batch:580, Cost:10.30472, Time:558\n",
      "Pass:0, Batch:590, Cost:10.30470, Time:567\n",
      "Pass:0, Batch:600, Cost:10.30465, Time:576\n",
      "Pass:0, Batch:610, Cost:10.30453, Time:586\n",
      "Pass:0, Batch:620, Cost:10.30453, Time:596\n",
      "Pass:0, Batch:630, Cost:10.30456, Time:605\n",
      "Pass:0, Batch:640, Cost:10.30437, Time:615\n",
      "Pass:0, Batch:650, Cost:10.30444, Time:624\n",
      "Pass:0, Batch:660, Cost:10.30432, Time:634\n",
      "Pass:0, Batch:670, Cost:10.30427, Time:643\n",
      "Pass:0, Batch:680, Cost:10.30418, Time:653\n",
      "Pass:0, Batch:690, Cost:10.30413, Time:662\n",
      "Pass:0, Batch:700, Cost:10.30404, Time:672\n",
      "Pass:0, Batch:710, Cost:10.30407, Time:681\n",
      "Pass:0, Batch:720, Cost:10.30393, Time:691\n",
      "Pass:0, Batch:730, Cost:10.30397, Time:701\n",
      "Pass:0, Batch:740, Cost:10.30399, Time:710\n",
      "Pass:0, Batch:750, Cost:10.30385, Time:719\n",
      "Pass:0, Batch:760, Cost:10.30384, Time:729\n",
      "Pass:0, Batch:770, Cost:10.30379, Time:739\n",
      "Pass:0, Batch:780, Cost:10.30368, Time:748\n",
      "Pass:0, Batch:790, Cost:10.30374, Time:758\n",
      "Pass:0, Batch:800, Cost:10.30361, Time:767\n",
      "Pass:0, Batch:810, Cost:10.30340, Time:777\n",
      "Pass:0, Batch:820, Cost:10.30354, Time:787\n",
      "Pass:0, Batch:830, Cost:10.30338, Time:796\n",
      "Pass:0, Batch:840, Cost:10.30329, Time:805\n",
      "Pass:0, Batch:850, Cost:10.30329, Time:815\n",
      "Pass:0, Batch:860, Cost:10.30325, Time:824\n",
      "Pass:0, Batch:870, Cost:10.30334, Time:834\n",
      "Pass:0, Batch:880, Cost:10.30301, Time:843\n",
      "Pass:0, Batch:890, Cost:10.30316, Time:852\n",
      "Pass:0, Batch:900, Cost:10.30311, Time:862\n",
      "Pass:0, Batch:910, Cost:10.30305, Time:871\n",
      "Pass:0, Batch:920, Cost:10.30291, Time:881\n",
      "Pass:0, Batch:930, Cost:10.30293, Time:890\n",
      "Pass:0, Batch:940, Cost:10.30287, Time:900\n",
      "Pass:0, Batch:950, Cost:10.30279, Time:909\n",
      "save models to machine_translation.inference.model\n"
     ]
    }
   ],
   "source": [
    "## 自行配置合适的EPOCH_NUM数值，并完成训练。\n",
    "import time\n",
    "\n",
    "EPOCH_NUM = 1\n",
    "start_time = time.time()\n",
    "\n",
    "for pass_id in range(EPOCH_NUM):\n",
    "\n",
    "     # 进行训练\n",
    "\n",
    "    train_cost = 0\n",
    "\n",
    "    for batch_id, data in enumerate(train_reader()):                  #遍历train_reader迭代器\n",
    "\n",
    "        train_cost = exe.run(program=fluid.default_main_program(),    #运行主程序\n",
    "\n",
    "                             feed=feeder.feed(data),                  #喂入一个batch的数据\n",
    "\n",
    "                             fetch_list=[avg_cost])                   #fetch平均误差\n",
    "\n",
    "\n",
    "        if batch_id % 10 == 0:                                        #每10次batch打印一次训练误差\n",
    "\n",
    "            print('Pass:%d, Batch:%d, Cost:%0.5f, Time:%d' % \n",
    "            (pass_id, batch_id, train_cost[0], time.time()-start_time))\n",
    "        \n",
    "        \n",
    "            \n",
    "    #保存模型\n",
    "    #model_save_dir = \"/home/aistudio/data/machinetranslet.inference.model\"\n",
    "    #如果保存路径不在就创建\n",
    "\n",
    "    if not os.path.exists(model_save_dir):\n",
    "\n",
    "        os.makedirs(model_save_dir)\n",
    "\n",
    "        print ('save models to %s' % (model_save_dir))\n",
    "\n",
    "        #修改指定的 main_program ，构建一个专门用于预测的 Program，\n",
    "        #然后 executor 把它和所有相关参数保存到 dirname 中。\n",
    "        fluid.io.save_inference_model(model_save_dir, ['src_word_id'], [rnn_out], exe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **4、模型预测**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "###预测阶段\n",
    "\n",
    "###\n",
    "\n",
    "#导入需要的包\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import paddle as paddle\n",
    "\n",
    "import paddle.fluid as fluid\n",
    "\n",
    "import os\n",
    "\n",
    "import paddle.fluid.layers as pd\n",
    "\n",
    "\n",
    "\n",
    "dict_size = 30000\n",
    "\n",
    "source_dict_dim = target_dict_dim = dict_size\n",
    "\n",
    "hidden_dim = 32\n",
    "\n",
    "word_dim = 32\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "max_length = 8\n",
    "\n",
    "beam_size = 2\n",
    "\n",
    "is_sparse = True\n",
    "\n",
    "decoder_size = hidden_dim\n",
    "\n",
    "model_save_dir = \"machine_translation.inference.model\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#预测阶段编码器\n",
    "\n",
    "def encoder():\n",
    "\n",
    "    src_word_id = pd.data(\n",
    "\n",
    "        name=\"src_word_id\", shape=[1], dtype='int64', lod_level=1)\n",
    "\n",
    "    src_embedding = pd.embedding(\n",
    "\n",
    "        input=src_word_id,\n",
    "\n",
    "        size=[dict_size, word_dim],\n",
    "\n",
    "        dtype='float32',\n",
    "\n",
    "        is_sparse=is_sparse,\n",
    "\n",
    "        param_attr=fluid.ParamAttr(name='vemb'))\n",
    "\n",
    "\n",
    "\n",
    "    fc1 = pd.fc(input=src_embedding, size=hidden_dim * 4, act='tanh')\n",
    "\n",
    "    lstm_hidden0, lstm_0 = pd.dynamic_lstm(input=fc1, size=hidden_dim * 4)\n",
    "\n",
    "    encoder_out = pd.sequence_last_step(input=lstm_hidden0)\n",
    "\n",
    "    return encoder_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "假设字典为[a,b,c]，beam size选择2，则如下图有：\n",
    "\n",
    "1、在生成第1个词的时候，选择概率最大的2个词，那么当前序列就是a或b;\n",
    "\n",
    "2、生成第2个词的时候，我们将当前序列a或b，分别与字典中的所有词进行组合，得到新的6个序列aa ab ac ba bb bc,然后从其中选择2个概率最高的，作为当前序列，即ab或bb;\n",
    "\n",
    "3、不断重复这个过程，直到遇到结束符为止。最终输出2个概率最高的序列。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/c78c01d4a57940848e135173a706d374fa5411ff3d2945e4958e40f4ef0ffa94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py:1520: UserWarning: There are no operators in the program to be executed. If you pass Program manually, please use fluid.program_guard to ensure the current Program is being used.\n",
      "  warnings.warn(error_info)\n",
      "W0113 13:23:25.462798  1860 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.2\n",
      "W0113 13:23:25.467738  1860 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      "&quot; Il y aura beaucoup d &apos; instituts de ce type et il y aura une Europe vraiment à deux vitesses &quot; , s&apos; attend Madame <unk> .\n",
      "Translated score and sentence:\n",
      "1\t-9.3016\t<e>\n",
      "\n",
      "2\t-19.6033\t, <e>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paddle.enable_static()\n",
    "\n",
    "#预测阶段解码器\n",
    "\n",
    "def decode(context):\n",
    "\n",
    "    init_state = context\n",
    "\n",
    "    #创建一个张量，值为目标序列最大长度max_length\n",
    "    array_len = pd.fill_constant(shape=[1], dtype='int64', value=max_length)\n",
    "\n",
    "    counter = pd.zeros(shape=[1], dtype='int64', force_cpu=True)\n",
    "\n",
    "\n",
    "\n",
    "    # fill the first element with init_state\n",
    "\n",
    "    state_array = pd.create_array('float32')\n",
    "\n",
    "    pd.array_write(init_state, array=state_array, i=counter)\n",
    "\n",
    "\n",
    "\n",
    "    # ids, scores as memory\n",
    "    #用两个array存储生成token的id，以及累计得分\n",
    "\n",
    "    ids_array = pd.create_array('int64')\n",
    "\n",
    "    scores_array = pd.create_array('float32')\n",
    "\n",
    "\n",
    "    #初始id和初始得分\n",
    "    init_ids = pd.data(name=\"init_ids\", shape=[1], dtype=\"int64\", lod_level=2)\n",
    "\n",
    "    init_scores = pd.data(\n",
    "\n",
    "        name=\"init_scores\", shape=[1], dtype=\"float32\", lod_level=2)\n",
    "\n",
    "\n",
    "    #初始id和初始得分写入对应array\n",
    "    pd.array_write(init_ids, array=ids_array, i=counter)\n",
    "\n",
    "    pd.array_write(init_scores, array=scores_array, i=counter)\n",
    "\n",
    "\n",
    "    #生成目标语言长度大于等于max_length则停止解码\n",
    "    cond = pd.less_than(x=counter, y=array_len)\n",
    "    while_op = pd.While(cond=cond)\n",
    "\n",
    "    with while_op.block():\n",
    "\n",
    "        #从对应array中读取上一时刻的id, state, score\n",
    "        pre_ids = pd.array_read(array=ids_array, i=counter)\n",
    "\n",
    "        pre_state = pd.array_read(array=state_array, i=counter)\n",
    "\n",
    "        pre_score = pd.array_read(array=scores_array, i=counter)\n",
    "\n",
    "\n",
    "        # expand the lod of pre_state to be the same with pre_score\n",
    "\n",
    "        pre_state_expanded = pd.sequence_expand(pre_state, pre_score)\n",
    "\n",
    "\n",
    "\n",
    "        pre_ids_emb = pd.embedding(\n",
    "\n",
    "            input=pre_ids,\n",
    "\n",
    "            size=[dict_size, word_dim],\n",
    "\n",
    "            dtype='float32',\n",
    "\n",
    "            is_sparse=is_sparse,\n",
    "\n",
    "            param_attr=fluid.ParamAttr(name='vemb'))\n",
    "\n",
    "\n",
    "\n",
    "        # use rnn unit to update rnn\n",
    "\n",
    "        current_state = pd.fc(\n",
    "\n",
    "            input=[pre_state_expanded, pre_ids_emb],\n",
    "\n",
    "            size=decoder_size,\n",
    "\n",
    "            act='tanh')\n",
    "\n",
    "        current_state_with_lod = pd.lod_reset(x=current_state, y=pre_score)\n",
    "\n",
    "        ################################\n",
    "\n",
    "        # use score to do beam search\n",
    "\n",
    "        ################################\n",
    "\n",
    "        current_score = pd.fc(\n",
    "\n",
    "            input=current_state_with_lod, size=target_dict_dim, act='softmax')\n",
    "\n",
    "        topk_scores, topk_indices = pd.topk(current_score, k=beam_size)\n",
    "\n",
    "        # calculate accumulated scores after topk to reduce computation cost\n",
    "\n",
    "        accu_scores = pd.elementwise_add(\n",
    "\n",
    "            x=pd.log(topk_scores), y=pd.reshape(pre_score, shape=[-1]), axis=0)\n",
    "\n",
    "        #根据beam_search算法，选出当前时刻得分最高的topk个ids，以及对应得分\n",
    "        selected_ids, selected_scores = pd.beam_search(\n",
    "\n",
    "            pre_ids,\n",
    "\n",
    "            pre_score,\n",
    "\n",
    "            topk_indices,\n",
    "\n",
    "            accu_scores,\n",
    "\n",
    "            beam_size,\n",
    "\n",
    "            end_id=1,\n",
    "\n",
    "            level=0)\n",
    "\n",
    "\n",
    "        #多条件分支语句\n",
    "        with pd.Switch() as switch:\n",
    "\n",
    "            #若所有beam分支的句子都解码结束，设cond=0，终止循环\n",
    "            with switch.case(pd.is_empty(selected_ids)):\n",
    "\n",
    "                pd.fill_constant(\n",
    "\n",
    "                    shape=[1], value=0, dtype='bool', force_cpu=True, out=cond)\n",
    "\n",
    "            with switch.default():\n",
    "\n",
    "                pd.increment(x=counter, value=1, in_place=True)\n",
    "\n",
    "\n",
    "\n",
    "                # update the memories\n",
    "\n",
    "                pd.array_write(current_state, array=state_array, i=counter)\n",
    "\n",
    "                pd.array_write(selected_ids, array=ids_array, i=counter)\n",
    "\n",
    "                pd.array_write(selected_scores, array=scores_array, i=counter)\n",
    "\n",
    "\n",
    "\n",
    "                # update the break condition: up to the max length or all candidates of\n",
    "\n",
    "                # source sentences have ended.\n",
    "\n",
    "                length_cond = pd.less_than(x=counter, y=array_len)\n",
    "\n",
    "                finish_cond = pd.logical_not(pd.is_empty(x=selected_ids))\n",
    "\n",
    "                pd.logical_and(x=length_cond, y=finish_cond, out=cond)\n",
    "\n",
    "\n",
    "    #beam_search_decode返回两个LoDTensor，它们lod的level=2，\n",
    "    #这两个level分别表示每个源句有多少个假设，每个假设有多少个id\n",
    "    translation_ids, translation_scores = pd.beam_search_decode(\n",
    "\n",
    "        ids=ids_array, scores=scores_array, beam_size=beam_size, end_id=1)\n",
    "\n",
    "\n",
    "    return translation_ids, translation_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def decode_main(use_cuda):\n",
    "\n",
    "    # 配置运算场所，定义使用CPU还是GPU，使用CPU时use_cuda = False，使用GPU时use_cuda = True\n",
    "\n",
    "    place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n",
    "\n",
    "    exe = fluid.Executor(place)\n",
    "\n",
    "    # 进行参数初始化\n",
    "\n",
    "    exe.run(fluid.default_startup_program())\n",
    "\n",
    "\n",
    "\n",
    "    context = encoder()\n",
    "\n",
    "    translation_ids, translation_scores = decode(context)\n",
    "\n",
    "    #该函数从给定 main_program 中取出所有 persistable==True 的变量（即长期变量），\n",
    "    #然后将它们从目录 dirname 中或 filename 指定的文件中加载出来\n",
    "    #注意:有些变量不是参数，但它们对于训练是必要的。因此，调用 \n",
    "    #save_params() 和 load_params() 来保存和加载参数是不够的，可以使用 \n",
    "    #save_persistables() 和 load_persistables() 代替这两个函数。\n",
    "    fluid.io.load_persistables(executor=exe, dirname=model_save_dir)\n",
    "    \n",
    "    \n",
    "    #初始id，和初始score\n",
    "    init_ids_data = np.array([0 for _ in range(batch_size)], dtype='int64')\n",
    "\n",
    "    init_scores_data = np.array(\n",
    "\n",
    "        [1. for _ in range(batch_size)], dtype='float32')\n",
    "\n",
    "    init_ids_data = init_ids_data.reshape((batch_size, 1))\n",
    "\n",
    "    init_scores_data = init_scores_data.reshape((batch_size, 1))\n",
    "\n",
    "    init_lod = [1] * batch_size\n",
    "\n",
    "    #lod_level=2\n",
    "    init_lod = [init_lod, init_lod]\n",
    "\n",
    "\n",
    "\n",
    "    init_ids = fluid.create_lod_tensor(init_ids_data, init_lod, place)\n",
    "\n",
    "    init_scores = fluid.create_lod_tensor(init_scores_data, init_lod, place)\n",
    "    \n",
    "\n",
    "\n",
    "    test_reader = paddle.batch(\n",
    "\n",
    "        paddle.reader.shuffle(\n",
    "\n",
    "            paddle.dataset.wmt14.test(dict_size), buf_size=1000),\n",
    "\n",
    "        batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "    feed_order = ['src_word_id']\n",
    "\n",
    "    feed_list = [\n",
    "        \n",
    "        #获取变量名为var_name(此处为src_word_id)的变量\n",
    "        fluid.default_main_program().global_block().var(var_name)\n",
    "\n",
    "        for var_name in feed_order\n",
    "\n",
    "    ]\n",
    "    \n",
    "\n",
    "    feeder = fluid.DataFeeder(feed_list, place)\n",
    "\n",
    "\n",
    "\n",
    "    src_dict, trg_dict = paddle.dataset.wmt14.get_dict(dict_size)\n",
    "\n",
    "\n",
    "\n",
    "    #开始预测结果\n",
    "\n",
    "\n",
    "    for data in test_reader():\n",
    "\n",
    "        #测试阶段feed_dict中的keys包含src_word_id, init_ids, init_scores\n",
    "        feed_data = list(map(lambda x: [x[0]], data))\n",
    "\n",
    "        feed_dict = feeder.feed(feed_data)\n",
    "\n",
    "        feed_dict['init_ids'] = init_ids\n",
    "\n",
    "        feed_dict['init_scores'] = init_scores\n",
    "\n",
    "        results = exe.run(\n",
    "\n",
    "            fluid.default_main_program(),\n",
    "\n",
    "            feed=feed_dict,\n",
    "\n",
    "            fetch_list=[translation_ids, translation_scores],\n",
    "\n",
    "            return_numpy=False)\n",
    "\n",
    "\n",
    "\n",
    "        result_ids = np.array(results[0])\n",
    "\n",
    "        result_ids_lod = results[0].lod()\n",
    "\n",
    "        result_scores = np.array(results[1])\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Original sentence:\")\n",
    "\n",
    "        print(\" \".join([src_dict[w] for w in feed_data[0][0][1:-1]]))\n",
    "\n",
    "        print(\"Translated score and sentence:\")\n",
    "\n",
    "        #打印出mini_batch中第一条数据beam_search的beam_size个翻译结果\n",
    "        \n",
    "        for i in range(beam_size):\n",
    "\n",
    "            #根据LoD得出每个句子的开始/终置位置下标\n",
    "            start_pos = result_ids_lod[1][i] + 1\n",
    "\n",
    "            end_pos = result_ids_lod[1][i + 1]\n",
    "            \n",
    "\n",
    "            print(\"%d\\t%.4f\\t%s\\n\" % (\n",
    "\n",
    "                i + 1, result_scores[end_pos - 1],\n",
    "\n",
    "                \" \".join([trg_dict[w] for w in result_ids[start_pos:end_pos]])))\n",
    "                \n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "def main(use_cuda):\n",
    "\n",
    "    decode_main(use_cuda)  # Beam Search does not support CUDA\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
