{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **任务介绍**\n",
    "\n",
    "机器翻译：即用计算机实现从源语言到目标语言转换的过程，是自然语言处理的重要研究领域之一。\n",
    "\n",
    "源语言：被翻译的语言\n",
    "\n",
    "目标语言：翻译后的结果语言\n",
    "\n",
    "\n",
    "# **数据集介绍**\n",
    "\n",
    "**数据集：WMT-14数据集**\n",
    "\n",
    "    该数据集有193319条训练数据，6003条测试数据，词典长度为30000。\n",
    "\n",
    "    Paddle接口paddle.dataset.wmt14中默认提供了一个经过预处理的较小规模的数据集。\n",
    "\n",
    "数据预处理：\n",
    "\n",
    "    将每个源语言到目标语言的平行语料库文件合并为一个文件，合并每个xxx.src和xxx.trg文件为xxx；xxx中的第i行内容为xxx.src的第i行和xxx.trg中的第i行连接，用“t”分隔。\n",
    "\n",
    "    创建训练数据的源字典和目标字典。每个字典都有DICSIZE个单词，包括语料中词频最高的DICSIZE-3个单词和三个特殊符号：\n",
    "\n",
    "    < s >表示序列的开始\n",
    "    < e >表示序列的结束\n",
    "    < unk >表示未登录词\n",
    "\n",
    "# **实践流程**\n",
    "# **1、准备数据**\n",
    "# **2、配置网络**\n",
    "\n",
    "    定义网络\n",
    "    定义损失函数\n",
    "    定义优化算法\n",
    "\n",
    "# **3、训练网络**\n",
    "# **4、模型预测**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#导入需要的包\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import paddle as paddle\n",
    "\n",
    "import paddle.fluid as fluid\n",
    "\n",
    "import os\n",
    "\n",
    "import paddle.fluid.layers as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_size = 30000 #字典维度\n",
    "\n",
    "source_dict_dim = target_dict_dim = dict_size # source_dict_dim:源语言字典维度 target_dict_dim：目标语言字典维度\n",
    "\n",
    "hidden_dim = 32 #解码中隐层大小\n",
    "\n",
    "decoder_size = hidden_dim #解码中隐层大小\n",
    "\n",
    "word_dim = 32 #词向量维度\n",
    "\n",
    "batch_size = 20 #数据提供器每次读入的数据批次大小\n",
    "\n",
    "max_length = 8 #生成句子的最大长度\n",
    "\n",
    "beam_size = 2 #柱宽度\n",
    "\n",
    "is_sparse = True #代表是否用稀疏更新的标志\n",
    "\n",
    "model_save_dir = \"machine_translation.inference.model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **1、准备数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# *********获取训练数据读取器和测试数据读取器train_reader 和test_reader***************\n",
    "\n",
    "train_reader = paddle.batch(\n",
    "\n",
    "        paddle.reader.shuffle(\n",
    "\n",
    "            paddle.dataset.wmt14.train(dict_size),buf_size=1000),#dict_size:字典维度 buf_size:乱序时的缓存大小\n",
    "\n",
    "        batch_size=batch_size)                                   #batch_size:批次数据大小\n",
    "\n",
    "#加载预测的数据\n",
    "\n",
    "test_reader = paddle.batch(\n",
    "\n",
    "    paddle.reader.shuffle(\n",
    "\n",
    "        paddle.dataset.wmt14.test(dict_size), buf_size=1000),\n",
    "\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **编码器解码器框架**\n",
    "\n",
    "解决的问题：由任意一个长度的原序列到另一个长度的目标序列的变化问题\n",
    "\n",
    "    编码：将整个原序列表征成一个向量\n",
    "\n",
    "    解码：通过最大化预测序列概率，从中解码出整个目标序列\n",
    "\n",
    "**柱搜索算法**\n",
    "\n",
    "    启发式搜索算法：在图或树中搜索每一步的最优扩展节点\n",
    "    \n",
    "    贪心算法：每一步最优，全局不一定最优\n",
    "    \n",
    "    场景：解空间非常大，内存装不下所有展开解的系统\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/5586f825a40e4a14a0fe9b9bd10d98e5f74d15d98c9d420b8921c8061b1c898a)\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **2、配置网络**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#************************实现编码器*******************************\n",
    "\n",
    "def encoder():\n",
    "\n",
    "    #输入是一个文字序列，被表示成整型的序列。\n",
    "    #lod_level=1 if the input is sequential data, and otherwise lod_level=0.\n",
    "\n",
    "    src_word_id = pd.data(\n",
    "\n",
    "        name=\"src_word_id\", shape=[1], dtype='int64', lod_level=1)\n",
    "\n",
    "    #将上述编码映射到低维语言空间的词向量\n",
    "\n",
    "    src_embedding = pd.embedding(\n",
    "\n",
    "        input=src_word_id,         #输入为独热编码\n",
    "\n",
    "        size=[dict_size, word_dim],#dict_size:字典维度 word_dim：词向量维度\n",
    "\n",
    "        dtype='float32',           \n",
    "\n",
    "        is_sparse=is_sparse,       #代表是否用稀疏更新的标志\n",
    "\n",
    "        #ParamAttr类代表了参数的各种属性。\n",
    "        #比如learning rate（学习率）, regularization（正则化）, trainable（可训练性）, do_model_average(平均化模型)和参数初始化方法.\n",
    "        \n",
    "        param_attr=fluid.ParamAttr(name='vemb'))   #可学习的隐藏层权重的参数属性。\n",
    "\n",
    "\n",
    "\n",
    "    #输入门、遗忘门和输出门中对于输入词向量的全连接操作 (Wfx, Wix, Wox, Wcx)\n",
    "    #这些操作不包括在dynamic_lstm运算中，用户在LSTM operator之前选择全连接运算\n",
    "    \n",
    "    fc1 = pd.fc(input=src_embedding, size=hidden_dim * 4, act='tanh')\n",
    "\n",
    "    #初始化lstm网络，注意size=hidden_dim*4\n",
    "    #返回：隐藏状态(hidden state)，LSTM神经元状态，两者都是T*D维\n",
    "    \n",
    "    lstm_hidden0, lstm_0 = pd.dynamic_lstm(input=fc1, size=hidden_dim * 4)\n",
    "\n",
    "    #完成所有时间步内的lstm计算，得到编码的最终输出\n",
    "\n",
    "    encoder_out = pd.sequence_last_step(input=lstm_hidden0)\n",
    "\n",
    "    return encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#************************定义训练模式下的解码器*******************************\n",
    "\n",
    "def train_decoder(context):\n",
    "\n",
    "    #获取目标语言序列\n",
    "\n",
    "    trg_language_word = pd.data(\n",
    "\n",
    "        name=\"target_language_word\", shape=[1], dtype='int64', lod_level=1)\n",
    "\n",
    "    #获取目标语言的词向量\n",
    "\n",
    "    trg_embedding = pd.embedding(\n",
    "\n",
    "        input=trg_language_word,\n",
    "\n",
    "        size=[dict_size, word_dim],#dict_size:字典维度 word_dim：词向量维度\n",
    "\n",
    "        dtype='float32',\n",
    "\n",
    "        is_sparse=is_sparse,\n",
    "\n",
    "        param_attr=fluid.ParamAttr(name='vemb'))\n",
    "\n",
    "    rnn = pd.DynamicRNN()\n",
    "\n",
    "    #定义RNN每一步的运算\n",
    "    with rnn.block():\n",
    "\n",
    "        #current_word不是序列数据，lod_level=0; trg_embedding是序列数据，lod_level=1\n",
    "        current_word = rnn.step_input(trg_embedding)#current_word:当前节点的输入\n",
    "\n",
    "        pre_state = rnn.memory(init=context, need_reorder=True)#pre_state:上个节点的输出\n",
    "\n",
    "        current_state = pd.fc(\n",
    "\n",
    "            input=[current_word, pre_state], size=decoder_size, act='tanh')#得到当前节点的输出\n",
    "\n",
    "        #对可能输出的单词进行打分，再用softmax函数进行归一化得到当前节点的概率\n",
    "\n",
    "        current_score = pd.fc(input=current_state, size=target_dict_dim, act='softmax')\n",
    "\n",
    "        #更新当前节点的输出为上个节点的输出\n",
    "\n",
    "        rnn.update_memory(pre_state, current_state)\n",
    "\n",
    "        rnn.output(current_score)\n",
    "\n",
    "    return rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#得到编码器\n",
    "\n",
    "context = encoder()\n",
    "\n",
    "#得到解码器\n",
    "\n",
    "rnn_out = train_decoder(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#训练标签\n",
    "\n",
    "label = pd.data(name=\"target_language_next_word\", shape=[1], dtype='int64', lod_level=1)\n",
    "\n",
    "# 用交叉熵损失函数计算损失，并使用mean算子进行损失规约\n",
    "\n",
    "cost = pd.cross_entropy(input=rnn_out, label=label)\n",
    "\n",
    "avg_cost = pd.mean(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#定义优化方法\n",
    "\n",
    "optimizer= fluid.optimizer.Adagrad(\n",
    "\n",
    "        learning_rate=1e-4,\n",
    "\n",
    "        regularization=fluid.regularizer.L2DecayRegularizer(#正则化函数，L2正则化通过限制参数数值\n",
    "                                                            #来缓解过拟合\n",
    "\n",
    "            regularization_coeff=0.1))\n",
    "\n",
    "opts = optimizer.minimize(avg_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#*************创建Executor*********************\n",
    "\n",
    "# 配置运算场所，定义使用CPU还是GPU，使用CPU时use_cuda = False，使用GPU时use_cuda = True\n",
    "\n",
    "use_cuda = False\n",
    "\n",
    "place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n",
    "\n",
    "exe = fluid.Executor(place)\n",
    "\n",
    "# 进行参数初始化\n",
    "\n",
    "exe.run(fluid.default_startup_program())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#定义数据映射器，feed_list对应数据集里每行的三条数据\n",
    "\n",
    "feeder = fluid.DataFeeder( place=place,feed_list=[ 'src_word_id', \n",
    "\n",
    "                                                   'target_language_word', \n",
    "\n",
    "                                                   'target_language_next_word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **3、训练网络**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "EPOCH_NUM = 1\n",
    "start_time = time.time()\n",
    "for pass_id in range(EPOCH_NUM):\n",
    "\n",
    "     # 进行训练\n",
    "\n",
    "    train_cost = 0\n",
    "\n",
    "    for batch_id, data in enumerate(train_reader()):                  #遍历train_reader迭代器\n",
    "\n",
    "        train_cost = exe.run(program=fluid.default_main_program(),    #运行主程序\n",
    "\n",
    "                             feed=feeder.feed(data),                  #喂入一个batch的数据\n",
    "\n",
    "                             fetch_list=[avg_cost])                   #fetch平均误差\n",
    "\n",
    "\n",
    "        if batch_id % 10 == 0:                                        #每10次batch打印一次训练误差\n",
    "\n",
    "            print('Pass:%d, Batch:%d, Cost:%0.5f, Time:%d' % \n",
    "            (pass_id, batch_id, train_cost[0], time.time()-start_time))\n",
    "            \n",
    "            # if not os.path.exists(model_save_dir):\n",
    "\n",
    "            #     os.makedirs(model_save_dir)\n",
    "\n",
    "            #     print ('save models to %s' % (model_save_dir))\n",
    "\n",
    "            #     #修改指定的 main_program ，构建一个专门用于预测的 Program，\n",
    "            #     #然后 executor 把它和所有相关参数保存到 dirname 中。\n",
    "            #     fluid.io.save_inference_model(model_save_dir, ['src_word_id'], [rnn_out], exe)\n",
    "            # break\n",
    "            \n",
    "    #保存模型\n",
    "    #model_save_dir = \"/home/aistudio/data/machinetranslet.inference.model\"\n",
    "    #如果保存路径不在就创建\n",
    "\n",
    "    if not os.path.exists(model_save_dir):\n",
    "\n",
    "        os.makedirs(model_save_dir)\n",
    "\n",
    "        print ('save models to %s' % (model_save_dir))\n",
    "\n",
    "        #修改指定的 main_program ，构建一个专门用于预测的 Program，\n",
    "        #然后 executor 把它和所有相关参数保存到 dirname 中。\n",
    "        fluid.io.save_inference_model(model_save_dir, ['src_word_id'], [rnn_out], exe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **4、模型预测**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "###预测阶段\n",
    "\n",
    "###\n",
    "\n",
    "#导入需要的包\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import paddle as paddle\n",
    "\n",
    "import paddle.fluid as fluid\n",
    "\n",
    "import os\n",
    "\n",
    "import paddle.fluid.layers as pd\n",
    "\n",
    "\n",
    "\n",
    "dict_size = 30000\n",
    "\n",
    "source_dict_dim = target_dict_dim = dict_size\n",
    "\n",
    "hidden_dim = 32\n",
    "\n",
    "word_dim = 32\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "max_length = 8\n",
    "\n",
    "beam_size = 2\n",
    "\n",
    "is_sparse = True\n",
    "\n",
    "decoder_size = hidden_dim\n",
    "\n",
    "model_save_dir = \"machine_translation.inference.model\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#预测阶段编码器\n",
    "\n",
    "def encoder():\n",
    "\n",
    "    src_word_id = pd.data(\n",
    "\n",
    "        name=\"src_word_id\", shape=[1], dtype='int64', lod_level=1)\n",
    "\n",
    "    src_embedding = pd.embedding(\n",
    "\n",
    "        input=src_word_id,\n",
    "\n",
    "        size=[dict_size, word_dim],\n",
    "\n",
    "        dtype='float32',\n",
    "\n",
    "        is_sparse=is_sparse,\n",
    "\n",
    "        param_attr=fluid.ParamAttr(name='vemb'))\n",
    "\n",
    "\n",
    "\n",
    "    fc1 = pd.fc(input=src_embedding, size=hidden_dim * 4, act='tanh')\n",
    "\n",
    "    lstm_hidden0, lstm_0 = pd.dynamic_lstm(input=fc1, size=hidden_dim * 4)\n",
    "\n",
    "    encoder_out = pd.sequence_last_step(input=lstm_hidden0)\n",
    "\n",
    "    return encoder_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "假设字典为[a,b,c]，beam size选择2，则如下图有：\n",
    "\n",
    "1、在生成第1个词的时候，选择概率最大的2个词，那么当前序列就是a或b;\n",
    "\n",
    "2、生成第2个词的时候，我们将当前序列a或b，分别与字典中的所有词进行组合，得到新的6个序列aa ab ac ba bb bc,然后从其中选择2个概率最高的，作为当前序列，即ab或bb;\n",
    "\n",
    "3、不断重复这个过程，直到遇到结束符为止。最终输出2个概率最高的序列。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/c78c01d4a57940848e135173a706d374fa5411ff3d2945e4958e40f4ef0ffa94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#预测阶段解码器\n",
    "\n",
    "def decode(context):\n",
    "\n",
    "    init_state = context\n",
    "\n",
    "    #创建一个张量，值为目标序列最大长度max_length\n",
    "    array_len = pd.fill_constant(shape=[1], dtype='int64', value=max_length)\n",
    "\n",
    "    counter = pd.zeros(shape=[1], dtype='int64', force_cpu=True)\n",
    "\n",
    "\n",
    "\n",
    "    # fill the first element with init_state\n",
    "\n",
    "    state_array = pd.create_array('float32')\n",
    "\n",
    "    pd.array_write(init_state, array=state_array, i=counter)\n",
    "\n",
    "\n",
    "\n",
    "    # ids, scores as memory\n",
    "    #用两个array存储生成token的id，以及累计得分\n",
    "\n",
    "    ids_array = pd.create_array('int64')\n",
    "\n",
    "    scores_array = pd.create_array('float32')\n",
    "\n",
    "\n",
    "    #初始id和初始得分\n",
    "    init_ids = pd.data(name=\"init_ids\", shape=[1], dtype=\"int64\", lod_level=2)\n",
    "\n",
    "    init_scores = pd.data(\n",
    "\n",
    "        name=\"init_scores\", shape=[1], dtype=\"float32\", lod_level=2)\n",
    "\n",
    "\n",
    "    #初始id和初始得分写入对应array\n",
    "    pd.array_write(init_ids, array=ids_array, i=counter)\n",
    "\n",
    "    pd.array_write(init_scores, array=scores_array, i=counter)\n",
    "\n",
    "\n",
    "    #生成目标语言长度大于等于max_length则停止解码\n",
    "    cond = pd.less_than(x=counter, y=array_len)\n",
    "    while_op = pd.While(cond=cond)\n",
    "\n",
    "    with while_op.block():\n",
    "\n",
    "        #从对应array中读取上一时刻的id, state, score\n",
    "        pre_ids = pd.array_read(array=ids_array, i=counter)\n",
    "\n",
    "        pre_state = pd.array_read(array=state_array, i=counter)\n",
    "\n",
    "        pre_score = pd.array_read(array=scores_array, i=counter)\n",
    "\n",
    "\n",
    "        # expand the lod of pre_state to be the same with pre_score\n",
    "\n",
    "        pre_state_expanded = pd.sequence_expand(pre_state, pre_score)\n",
    "\n",
    "\n",
    "\n",
    "        pre_ids_emb = pd.embedding(\n",
    "\n",
    "            input=pre_ids,\n",
    "\n",
    "            size=[dict_size, word_dim],\n",
    "\n",
    "            dtype='float32',\n",
    "\n",
    "            is_sparse=is_sparse,\n",
    "\n",
    "            param_attr=fluid.ParamAttr(name='vemb'))\n",
    "\n",
    "\n",
    "\n",
    "        # use rnn unit to update rnn\n",
    "\n",
    "        current_state = pd.fc(\n",
    "\n",
    "            input=[pre_state_expanded, pre_ids_emb],\n",
    "\n",
    "            size=decoder_size,\n",
    "\n",
    "            act='tanh')\n",
    "\n",
    "        current_state_with_lod = pd.lod_reset(x=current_state, y=pre_score)\n",
    "\n",
    "        ################################\n",
    "\n",
    "        # use score to do beam search\n",
    "\n",
    "        ################################\n",
    "\n",
    "        current_score = pd.fc(\n",
    "\n",
    "            input=current_state_with_lod, size=target_dict_dim, act='softmax')\n",
    "\n",
    "        topk_scores, topk_indices = pd.topk(current_score, k=beam_size)\n",
    "\n",
    "        # calculate accumulated scores after topk to reduce computation cost\n",
    "\n",
    "        accu_scores = pd.elementwise_add(\n",
    "\n",
    "            x=pd.log(topk_scores), y=pd.reshape(pre_score, shape=[-1]), axis=0)\n",
    "\n",
    "        #根据beam_search算法，选出当前时刻得分最高的topk个ids，以及对应得分\n",
    "        selected_ids, selected_scores = pd.beam_search(\n",
    "\n",
    "            pre_ids,\n",
    "\n",
    "            pre_score,\n",
    "\n",
    "            topk_indices,\n",
    "\n",
    "            accu_scores,\n",
    "\n",
    "            beam_size,\n",
    "\n",
    "            end_id=1,\n",
    "\n",
    "            level=0)\n",
    "\n",
    "\n",
    "        #多条件分支语句\n",
    "        with pd.Switch() as switch:\n",
    "\n",
    "            #若所有beam分支的句子都解码结束，设cond=0，终止循环\n",
    "            with switch.case(pd.is_empty(selected_ids)):\n",
    "\n",
    "                pd.fill_constant(\n",
    "\n",
    "                    shape=[1], value=0, dtype='bool', force_cpu=True, out=cond)\n",
    "\n",
    "            with switch.default():\n",
    "\n",
    "                pd.increment(x=counter, value=1, in_place=True)\n",
    "\n",
    "\n",
    "\n",
    "                # update the memories\n",
    "\n",
    "                pd.array_write(current_state, array=state_array, i=counter)\n",
    "\n",
    "                pd.array_write(selected_ids, array=ids_array, i=counter)\n",
    "\n",
    "                pd.array_write(selected_scores, array=scores_array, i=counter)\n",
    "\n",
    "\n",
    "\n",
    "                # update the break condition: up to the max length or all candidates of\n",
    "\n",
    "                # source sentences have ended.\n",
    "\n",
    "                length_cond = pd.less_than(x=counter, y=array_len)\n",
    "\n",
    "                finish_cond = pd.logical_not(pd.is_empty(x=selected_ids))\n",
    "\n",
    "                pd.logical_and(x=length_cond, y=finish_cond, out=cond)\n",
    "\n",
    "\n",
    "    #beam_search_decode返回两个LoDTensor，它们lod的level=2，\n",
    "    #这两个level分别表示每个源句有多少个假设，每个假设有多少个id\n",
    "    translation_ids, translation_scores = pd.beam_search_decode(\n",
    "\n",
    "        ids=ids_array, scores=scores_array, beam_size=beam_size, end_id=1)\n",
    "\n",
    "\n",
    "    return translation_ids, translation_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def decode_main(use_cuda):\n",
    "\n",
    "    # 配置运算场所，定义使用CPU还是GPU，使用CPU时use_cuda = False，使用GPU时use_cuda = True\n",
    "\n",
    "    place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n",
    "\n",
    "    exe = fluid.Executor(place)\n",
    "\n",
    "    # 进行参数初始化\n",
    "\n",
    "    exe.run(fluid.default_startup_program())\n",
    "\n",
    "\n",
    "\n",
    "    context = encoder()\n",
    "\n",
    "    translation_ids, translation_scores = decode(context)\n",
    "\n",
    "    #该函数从给定 main_program 中取出所有 persistable==True 的变量（即长期变量），\n",
    "    #然后将它们从目录 dirname 中或 filename 指定的文件中加载出来\n",
    "    #注意:有些变量不是参数，但它们对于训练是必要的。因此，调用 \n",
    "    #save_params() 和 load_params() 来保存和加载参数是不够的，可以使用 \n",
    "    #save_persistables() 和 load_persistables() 代替这两个函数。\n",
    "    fluid.io.load_persistables(executor=exe, dirname=model_save_dir)\n",
    "    \n",
    "    \n",
    "    #初始id，和初始score\n",
    "    init_ids_data = np.array([0 for _ in range(batch_size)], dtype='int64')\n",
    "\n",
    "    init_scores_data = np.array(\n",
    "\n",
    "        [1. for _ in range(batch_size)], dtype='float32')\n",
    "\n",
    "    init_ids_data = init_ids_data.reshape((batch_size, 1))\n",
    "\n",
    "    init_scores_data = init_scores_data.reshape((batch_size, 1))\n",
    "\n",
    "    init_lod = [1] * batch_size\n",
    "\n",
    "    #lod_level=2\n",
    "    init_lod = [init_lod, init_lod]\n",
    "\n",
    "\n",
    "\n",
    "    init_ids = fluid.create_lod_tensor(init_ids_data, init_lod, place)\n",
    "\n",
    "    init_scores = fluid.create_lod_tensor(init_scores_data, init_lod, place)\n",
    "    \n",
    "\n",
    "\n",
    "    test_reader = paddle.batch(\n",
    "\n",
    "        paddle.reader.shuffle(\n",
    "\n",
    "            paddle.dataset.wmt14.test(dict_size), buf_size=1000),\n",
    "\n",
    "        batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "    feed_order = ['src_word_id']\n",
    "\n",
    "    feed_list = [\n",
    "        \n",
    "        #获取变量名为var_name(此处为src_word_id)的变量\n",
    "        fluid.default_main_program().global_block().var(var_name)\n",
    "\n",
    "        for var_name in feed_order\n",
    "\n",
    "    ]\n",
    "    \n",
    "\n",
    "    feeder = fluid.DataFeeder(feed_list, place)\n",
    "\n",
    "\n",
    "\n",
    "    src_dict, trg_dict = paddle.dataset.wmt14.get_dict(dict_size)\n",
    "\n",
    "\n",
    "\n",
    "    #开始预测结果\n",
    "\n",
    "\n",
    "    for data in test_reader():\n",
    "\n",
    "        #测试阶段feed_dict中的keys包含src_word_id, init_ids, init_scores\n",
    "        feed_data = list(map(lambda x: [x[0]], data))\n",
    "\n",
    "        feed_dict = feeder.feed(feed_data)\n",
    "\n",
    "        feed_dict['init_ids'] = init_ids\n",
    "\n",
    "        feed_dict['init_scores'] = init_scores\n",
    "\n",
    "        results = exe.run(\n",
    "\n",
    "            fluid.default_main_program(),\n",
    "\n",
    "            feed=feed_dict,\n",
    "\n",
    "            fetch_list=[translation_ids, translation_scores],\n",
    "\n",
    "            return_numpy=False)\n",
    "\n",
    "\n",
    "\n",
    "        result_ids = np.array(results[0])\n",
    "\n",
    "        result_ids_lod = results[0].lod()\n",
    "\n",
    "        result_scores = np.array(results[1])\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Original sentence:\")\n",
    "\n",
    "        print(\" \".join([src_dict[w] for w in feed_data[0][0][1:-1]]))\n",
    "\n",
    "        print(\"Translated score and sentence:\")\n",
    "\n",
    "        #打印出mini_batch中第一条数据beam_search的beam_size个翻译结果\n",
    "        \n",
    "        for i in range(beam_size):\n",
    "\n",
    "            #根据LoD得出每个句子的开始/终置位置下标\n",
    "            start_pos = result_ids_lod[1][i] + 1\n",
    "\n",
    "            end_pos = result_ids_lod[1][i + 1]\n",
    "            \n",
    "\n",
    "            print(\"%d\\t%.4f\\t%s\\n\" % (\n",
    "\n",
    "                i + 1, result_scores[end_pos - 1],\n",
    "\n",
    "                \" \".join([trg_dict[w] for w in result_ids[start_pos:end_pos]])))\n",
    "                \n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "def main(use_cuda):\n",
    "\n",
    "    decode_main(False)  # Beam Search does not support CUDA\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
